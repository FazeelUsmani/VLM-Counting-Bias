{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "synthetic_occlusion_title"
      },
      "source": [
        "# Vision-Language Model Counting Under Synthetic Occlusion\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/vlm-counting-bias/blob/main/notebooks/01_counting_occlusion_synthetic.ipynb)\n",
        "\n",
        "This notebook systematically evaluates VLM counting performance on synthetic images with controlled occlusion levels.\n",
        "\n",
        "## Objectives\n",
        "- Generate synthetic images with known object counts\n",
        "- Apply varying levels of occlusion (0%, 25%, 50%, 75%)\n",
        "- Evaluate multiple VLMs (GPT-4V, BLIP-2, LLaVA)\n",
        "- Analyze counting bias under different occlusion conditions\n",
        "\n",
        "## Setup Requirements\n",
        "- OpenAI API key for GPT-4V\n",
        "- HuggingFace token for model access\n",
        "- GPU recommended for local model inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install required packages in Colab\n",
        "!pip install openai transformers torch pillow opencv-python matplotlib pandas numpy plotly\n",
        "!pip install accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from PIL import Image, ImageDraw\n",
        "import cv2\n",
        "import json\n",
        "import base64\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Dependencies loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_section"
      },
      "source": [
        "## Configuration and API Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api_setup"
      },
      "outputs": [],
      "source": [
        "# API Configuration\n",
        "# Set your API keys here or as environment variables\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
        "HF_TOKEN = os.getenv('HF_TOKEN', '')\n",
        "\n",
        "# If running in Colab, you can set keys directly (not recommended for production)\n",
        "if not OPENAI_API_KEY:\n",
        "    from google.colab import userdata\n",
        "    try:\n",
        "        OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    except:\n",
        "        OPENAI_API_KEY = input(\"Enter OpenAI API Key: \")\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    try:\n",
        "        HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    except:\n",
        "        HF_TOKEN = input(\"Enter HuggingFace Token (optional): \") or None\n",
        "\n",
        "# Experiment configuration\n",
        "EXPERIMENT_CONFIG = {\n",
        "    'image_size': (512, 512),\n",
        "    'num_images_per_condition': 10,  # Reduced for demo, increase for full experiment\n",
        "    'object_counts': [3, 5, 7, 10],  # Number of objects to generate\n",
        "    'occlusion_levels': [0.0, 0.25, 0.50, 0.75],  # Percentage of objects occluded\n",
        "    'object_types': ['circles', 'rectangles'],\n",
        "    'colors': ['red', 'blue', 'green', 'yellow', 'purple'],\n",
        "    'models_to_test': ['GPT-4V', 'BLIP-2']  # Add 'LLaVA' if GPU available\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "for key, value in EXPERIMENT_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "synthetic_generation"
      },
      "source": [
        "## Synthetic Data Generation\n",
        "\n",
        "We generate synthetic images with known object counts and controllable occlusion patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_generator"
      },
      "outputs": [],
      "source": [
        "class SyntheticDataGenerator:\n",
        "    def __init__(self, image_size=(512, 512), colors=None):\n",
        "        self.image_size = image_size\n",
        "        self.colors = colors or ['red', 'blue', 'green', 'yellow', 'purple']\n",
        "        self.color_map = {\n",
        "            'red': (255, 0, 0),\n",
        "            'blue': (0, 0, 255),\n",
        "            'green': (0, 255, 0),\n",
        "            'yellow': (255, 255, 0),\n",
        "            'purple': (128, 0, 128)\n",
        "        }\n",
        "    \n",
        "    def generate_objects(self, num_objects, object_type='circles'):\n",
        "        \"\"\"Generate random object positions and properties\"\"\"\n",
        "        objects = []\n",
        "        width, height = self.image_size\n",
        "        \n",
        "        for i in range(num_objects):\n",
        "            # Random position (ensure objects don't go outside image)\n",
        "            if object_type == 'circles':\n",
        "                radius = np.random.randint(20, 50)\n",
        "                x = np.random.randint(radius, width - radius)\n",
        "                y = np.random.randint(radius, height - radius)\n",
        "                obj = {\n",
        "                    'type': 'circle',\n",
        "                    'x': x,\n",
        "                    'y': y,\n",
        "                    'radius': radius,\n",
        "                    'color': np.random.choice(self.colors)\n",
        "                }\n",
        "            else:  # rectangles\n",
        "                width_obj = np.random.randint(30, 80)\n",
        "                height_obj = np.random.randint(30, 80)\n",
        "                x = np.random.randint(0, width - width_obj)\n",
        "                y = np.random.randint(0, height - height_obj)\n",
        "                obj = {\n",
        "                    'type': 'rectangle',\n",
        "                    'x': x,\n",
        "                    'y': y,\n",
        "                    'width': width_obj,\n",
        "                    'height': height_obj,\n",
        "                    'color': np.random.choice(self.colors)\n",
        "                }\n",
        "            \n",
        "            objects.append(obj)\n",
        "        \n",
        "        return objects\n",
        "    \n",
        "    def draw_objects(self, objects):\n",
        "        \"\"\"Draw objects on image\"\"\"\n",
        "        image = Image.new('RGB', self.image_size, color='white')\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        \n",
        "        for obj in objects:\n",
        "            color = self.color_map[obj['color']]\n",
        "            \n",
        "            if obj['type'] == 'circle':\n",
        "                x, y, r = obj['x'], obj['y'], obj['radius']\n",
        "                draw.ellipse([x-r, y-r, x+r, y+r], fill=color)\n",
        "            elif obj['type'] == 'rectangle':\n",
        "                x, y, w, h = obj['x'], obj['y'], obj['width'], obj['height']\n",
        "                draw.rectangle([x, y, x+w, y+h], fill=color)\n",
        "        \n",
        "        return image\n",
        "    \n",
        "    def apply_occlusion(self, image, occlusion_level):\n",
        "        \"\"\"Apply random occlusion patches to image\"\"\"\n",
        "        if occlusion_level == 0:\n",
        "            return image\n",
        "        \n",
        "        image_copy = image.copy()\n",
        "        draw = ImageDraw.Draw(image_copy)\n",
        "        width, height = self.image_size\n",
        "        \n",
        "        # Calculate number of occlusion patches based on level\n",
        "        total_area = width * height\n",
        "        occlusion_area = total_area * occlusion_level\n",
        "        \n",
        "        # Generate random rectangular occlusion patches\n",
        "        patches_generated = 0\n",
        "        current_area = 0\n",
        "        \n",
        "        while current_area < occlusion_area and patches_generated < 20:  # Limit patches\n",
        "            # Random patch size\n",
        "            patch_w = np.random.randint(30, min(150, width // 3))\n",
        "            patch_h = np.random.randint(30, min(150, height // 3))\n",
        "            \n",
        "            # Random patch position\n",
        "            patch_x = np.random.randint(0, width - patch_w)\n",
        "            patch_y = np.random.randint(0, height - patch_h)\n",
        "            \n",
        "            # Draw black occlusion patch\n",
        "            draw.rectangle([patch_x, patch_y, patch_x + patch_w, patch_y + patch_h], \n",
        "                          fill=(0, 0, 0))\n",
        "            \n",
        "            current_area += patch_w * patch_h\n",
        "            patches_generated += 1\n",
        "        \n",
        "        return image_copy\n",
        "    \n",
        "    def generate_dataset(self, num_objects_list, occlusion_levels, \n",
        "                        num_images_per_condition, object_type='circles'):\n",
        "        \"\"\"Generate complete synthetic dataset\"\"\"\n",
        "        dataset = []\n",
        "        \n",
        "        for num_objects in num_objects_list:\n",
        "            for occlusion_level in occlusion_levels:\n",
        "                for img_idx in range(num_images_per_condition):\n",
        "                    # Generate objects\n",
        "                    objects = self.generate_objects(num_objects, object_type)\n",
        "                    \n",
        "                    # Draw base image\n",
        "                    base_image = self.draw_objects(objects)\n",
        "                    \n",
        "                    # Apply occlusion\n",
        "                    final_image = self.apply_occlusion(base_image, occlusion_level)\n",
        "                    \n",
        "                    # Create metadata\n",
        "                    metadata = {\n",
        "                        'image_id': f\"{object_type}_{num_objects}obj_{int(occlusion_level*100)}occ_{img_idx}\",\n",
        "                        'true_count': num_objects,\n",
        "                        'occlusion_level': occlusion_level,\n",
        "                        'object_type': object_type,\n",
        "                        'objects': objects,\n",
        "                        'image': final_image\n",
        "                    }\n",
        "                    \n",
        "                    dataset.append(metadata)\n",
        "        \n",
        "        return dataset\n",
        "\n",
        "# Initialize generator\n",
        "generator = SyntheticDataGenerator(\n",
        "    image_size=EXPERIMENT_CONFIG['image_size'],\n",
        "    colors=EXPERIMENT_CONFIG['colors']\n",
        ")\n",
        "\n",
        "print(\"Synthetic data generator initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_dataset"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic dataset\n",
        "print(\"Generating synthetic dataset...\")\n",
        "\n",
        "synthetic_dataset = []\n",
        "\n",
        "for object_type in EXPERIMENT_CONFIG['object_types']:\n",
        "    print(f\"\\nGenerating {object_type} dataset...\")\n",
        "    \n",
        "    dataset = generator.generate_dataset(\n",
        "        num_objects_list=EXPERIMENT_CONFIG['object_counts'],\n",
        "        occlusion_levels=EXPERIMENT_CONFIG['occlusion_levels'],\n",
        "        num_images_per_condition=EXPERIMENT_CONFIG['num_images_per_condition'],\n",
        "        object_type=object_type\n",
        "    )\n",
        "    \n",
        "    synthetic_dataset.extend(dataset)\n",
        "    print(f\"Generated {len(dataset)} images for {object_type}\")\n",
        "\n",
        "print(f\"\\nTotal synthetic images generated: {len(synthetic_dataset)}\")\n",
        "\n",
        "# Display sample images\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "sample_indices = np.linspace(0, len(synthetic_dataset)-1, 8, dtype=int)\n",
        "\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    sample = synthetic_dataset[idx]\n",
        "    axes[i].imshow(sample['image'])\n",
        "    axes[i].set_title(f\"{sample['object_type'].title()}\\nCount: {sample['true_count']}\\nOcclusion: {sample['occlusion_level']:.0%}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Sample Synthetic Images', y=1.02, fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlm_interface"
      },
      "source": [
        "## VLM Interface Implementation\n",
        "\n",
        "Implementation of interfaces for different Vision-Language Models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlm_implementation"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import requests\n",
        "import re\n",
        "\n",
        "class VLMInterface:\n",
        "    def __init__(self, openai_key=None, hf_token=None):\n",
        "        self.openai_key = openai_key\n",
        "        self.hf_token = hf_token\n",
        "        \n",
        "        if openai_key:\n",
        "            self.openai_client = openai.OpenAI(api_key=openai_key)\n",
        "    \n",
        "    def count_objects_gpt4v(self, image_base64, object_type, max_retries=3):\n",
        "        \"\"\"Count objects using GPT-4V\"\"\"\n",
        "        if not self.openai_key:\n",
        "            raise ValueError(\"OpenAI API key required for GPT-4V\")\n",
        "        \n",
        "        prompt = f\"\"\"Look at this image carefully and count the number of {object_type}.\n",
        "        \n",
        "        Please respond with ONLY a JSON object in this format:\n",
        "        {{\n",
        "            \"count\": <number>,\n",
        "            \"confidence\": <0.0 to 1.0>,\n",
        "            \"reasoning\": \"<brief explanation>\"\n",
        "        }}\n",
        "        \n",
        "        Be precise in your counting and provide an honest confidence score.\"\"\"\n",
        "        \n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                # the newest OpenAI model is \"gpt-4o\" which was released May 13, 2024.\n",
        "                # do not change this unless explicitly requested by the user\n",
        "                response = self.openai_client.chat.completions.create(\n",
        "                    model=\"gpt-4o\",\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": [\n",
        "                                {\"type\": \"text\", \"text\": prompt},\n",
        "                                {\n",
        "                                    \"type\": \"image_url\",\n",
        "                                    \"image_url\": {\n",
        "                                        \"url\": f\"data:image/png;base64,{image_base64}\"\n",
        "                                    }\n",
        "                                }\n",
        "                            ]\n",
        "                        }\n",
        "                    ],\n",
        "                    max_tokens=200,\n",
        "                    response_format={\"type\": \"json_object\"}\n",
        "                )\n",
        "                \n",
        "                result_text = response.choices[0].message.content\n",
        "                result = json.loads(result_text)\n",
        "                \n",
        "                return {\n",
        "                    'count': int(result.get('count', 0)),\n",
        "                    'confidence': float(result.get('confidence', 0.5)),\n",
        "                    'reasoning': result.get('reasoning', ''),\n",
        "                    'raw_response': result_text\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    return {\n",
        "                        'count': 0,\n",
        "                        'confidence': 0.0,\n",
        "                        'error': str(e),\n",
        "                        'reasoning': f'Error after {max_retries} attempts'\n",
        "                    }\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "    \n",
        "    def count_objects_blip2(self, image_base64, object_type, max_retries=3):\n",
        "        \"\"\"Count objects using BLIP-2 via HuggingFace Inference API\"\"\"\n",
        "        \n",
        "        # Using HuggingFace Inference API for BLIP-2\n",
        "        api_url = \"https://api-inference.huggingface.co/models/Salesforce/blip2-opt-2.7b\"\n",
        "        \n",
        "        headers = {}\n",
        "        if self.hf_token:\n",
        "            headers[\"Authorization\"] = f\"Bearer {self.hf_token}\"\n",
        "        \n",
        "        # Convert base64 to bytes\n",
        "        image_bytes = base64.b64decode(image_base64)\n",
        "        \n",
        "        # Create prompt for counting\n",
        "        question = f\"How many {object_type} are in this image?\"\n",
        "        \n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = requests.post(\n",
        "                    api_url,\n",
        "                    headers=headers,\n",
        "                    data=image_bytes,\n",
        "                    params={\"question\": question}\n",
        "                )\n",
        "                \n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    answer = result[0]['answer'] if isinstance(result, list) else result.get('answer', '')\n",
        "                    \n",
        "                    # Extract number from answer\n",
        "                    count = self.extract_number_from_text(answer)\n",
        "                    \n",
        "                    # Simple confidence estimation based on answer clarity\n",
        "                    confidence = 0.8 if any(str(i) in answer for i in range(20)) else 0.5\n",
        "                    \n",
        "                    return {\n",
        "                        'count': count,\n",
        "                        'confidence': confidence,\n",
        "                        'reasoning': answer,\n",
        "                        'raw_response': str(result)\n",
        "                    }\n",
        "                else:\n",
        "                    if attempt == max_retries - 1:\n",
        "                        return {\n",
        "                            'count': 0,\n",
        "                            'confidence': 0.0,\n",
        "                            'error': f'HTTP {response.status_code}: {response.text}',\n",
        "                            'reasoning': 'API request failed'\n",
        "                        }\n",
        "                    \n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    return {\n",
        "                        'count': 0,\n",
        "                        'confidence': 0.0,\n",
        "                        'error': str(e),\n",
        "                        'reasoning': f'Error after {max_retries} attempts'\n",
        "                    }\n",
        "                \n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "    \n",
        "    def extract_number_from_text(self, text):\n",
        "        \"\"\"Extract a number from text response\"\"\"\n",
        "        if not text:\n",
        "            return 0\n",
        "        \n",
        "        # Look for explicit numbers\n",
        "        numbers = re.findall(r'\\b\\d+\\b', text)\n",
        "        if numbers:\n",
        "            return int(numbers[0])\n",
        "        \n",
        "        # Look for written numbers\n",
        "        number_words = {\n",
        "            'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,\n",
        "            'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10,\n",
        "            'eleven': 11, 'twelve': 12, 'thirteen': 13, 'fourteen': 14, 'fifteen': 15\n",
        "        }\n",
        "        \n",
        "        text_lower = text.lower()\n",
        "        for word, num in number_words.items():\n",
        "            if word in text_lower:\n",
        "                return num\n",
        "        \n",
        "        # Default to 0 if no number found\n",
        "        return 0\n",
        "    \n",
        "    def count_objects(self, model_name, image_base64, object_type):\n",
        "        \"\"\"Main interface for counting objects with different models\"\"\"\n",
        "        if model_name == 'GPT-4V':\n",
        "            return self.count_objects_gpt4v(image_base64, object_type)\n",
        "        elif model_name == 'BLIP-2':\n",
        "            return self.count_objects_blip2(image_base64, object_type)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "# Initialize VLM interface\n",
        "vlm_interface = VLMInterface(openai_key=OPENAI_API_KEY, hf_token=HF_TOKEN)\n",
        "print(\"VLM interface initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experiment_execution"
      },
      "source": [
        "## Experiment Execution\n",
        "\n",
        "Run the systematic evaluation across all models and conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_experiment"
      },
      "outputs": [],
      "source": [
        "def run_counting_experiment(dataset, models_to_test, vlm_interface, sample_size=None):\n",
        "    \"\"\"Run counting experiment on synthetic dataset\"\"\"\n",
        "    \n",
        "    # Sample dataset if specified\n",
        "    if sample_size and sample_size < len(dataset):\n",
        "        dataset = np.random.choice(dataset, sample_size, replace=False)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(f\"Running experiment on {len(dataset)} images with {len(models_to_test)} models...\")\n",
        "    \n",
        "    progress_bar = tqdm(total=len(dataset) * len(models_to_test))\n",
        "    \n",
        "    for img_data in dataset:\n",
        "        # Convert image to base64\n",
        "        img_buffer = io.BytesIO()\n",
        "        img_data['image'].save(img_buffer, format='PNG')\n",
        "        img_base64 = base64.b64encode(img_buffer.getvalue()).decode()\n",
        "        \n",
        "        for model_name in models_to_test:\n",
        "            progress_bar.set_description(f\"Processing {img_data['image_id']} with {model_name}\")\n",
        "            \n",
        "            try:\n",
        "                # Get prediction from model\n",
        "                prediction = vlm_interface.count_objects(\n",
        "                    model_name, img_base64, img_data['object_type']\n",
        "                )\n",
        "                \n",
        "                # Store result\n",
        "                result = {\n",
        "                    'image_id': img_data['image_id'],\n",
        "                    'model': model_name,\n",
        "                    'true_count': img_data['true_count'],\n",
        "                    'predicted_count': prediction['count'],\n",
        "                    'confidence': prediction['confidence'],\n",
        "                    'occlusion_level': img_data['occlusion_level'],\n",
        "                    'object_type': img_data['object_type'],\n",
        "                    'error': prediction.get('error'),\n",
        "                    'reasoning': prediction.get('reasoning', ''),\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                \n",
        "                # Calculate metrics\n",
        "                if not prediction.get('error'):\n",
        "                    result['absolute_error'] = abs(prediction['count'] - img_data['true_count'])\n",
        "                    result['relative_error'] = result['absolute_error'] / max(img_data['true_count'], 1)\n",
        "                    result['bias'] = prediction['count'] - img_data['true_count']\n",
        "                    result['exact_match'] = prediction['count'] == img_data['true_count']\n",
        "                \n",
        "                results.append(result)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"\\nError processing {img_data['image_id']} with {model_name}: {str(e)}\")\n",
        "                \n",
        "                # Store error result\n",
        "                result = {\n",
        "                    'image_id': img_data['image_id'],\n",
        "                    'model': model_name,\n",
        "                    'true_count': img_data['true_count'],\n",
        "                    'predicted_count': 0,\n",
        "                    'confidence': 0.0,\n",
        "                    'occlusion_level': img_data['occlusion_level'],\n",
        "                    'object_type': img_data['object_type'],\n",
        "                    'error': str(e),\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                results.append(result)\n",
        "            \n",
        "            progress_bar.update(1)\n",
        "            \n",
        "            # Rate limiting delay\n",
        "            time.sleep(1)\n",
        "    \n",
        "    progress_bar.close()\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run the experiment (using a smaller sample for demo)\n",
        "print(\"Starting experiment...\")\n",
        "sample_size = min(20, len(synthetic_dataset))  # Limit for demo\n",
        "\n",
        "results_df = run_counting_experiment(\n",
        "    dataset=synthetic_dataset,\n",
        "    models_to_test=EXPERIMENT_CONFIG['models_to_test'],\n",
        "    vlm_interface=vlm_interface,\n",
        "    sample_size=sample_size\n",
        ")\n",
        "\n",
        "print(f\"\\nExperiment completed! Collected {len(results_df)} results.\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nSummary Statistics:\")\n",
        "successful_results = results_df[results_df['error'].isna()]\n",
        "print(f\"Successful predictions: {len(successful_results)}/{len(results_df)} ({len(successful_results)/len(results_df):.1%})\")\n",
        "\n",
        "if len(successful_results) > 0:\n",
        "    print(f\"Average absolute error: {successful_results['absolute_error'].mean():.2f}\")\n",
        "    print(f\"Exact match accuracy: {successful_results['exact_match'].mean():.1%}\")\n",
        "    print(f\"Average confidence: {successful_results['confidence'].mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis_section"
      },
      "source": [
        "## Results Analysis and Visualization\n",
        "\n",
        "Analyze the counting performance and visualize biases under different occlusion conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analyze_results"
      },
      "outputs": [],
      "source": [
        "# Filter successful results for analysis\n",
        "analysis_df = results_df[results_df['error'].isna()].copy()\n",
        "\n",
        "if len(analysis_df) == 0:\n",
        "    print(\"No successful results to analyze. Please check API configurations and try again.\")\n",
        "else:\n",
        "    print(f\"Analyzing {len(analysis_df)} successful predictions...\")\n",
        "    \n",
        "    # Performance by model\n",
        "    model_performance = analysis_df.groupby('model').agg({\n",
        "        'absolute_error': ['mean', 'std'],\n",
        "        'exact_match': 'mean',\n",
        "        'confidence': 'mean',\n",
        "        'bias': 'mean'\n",
        "    }).round(3)\n",
        "    \n",
        "    print(\"\\nModel Performance Summary:\")\n",
        "    print(model_performance)\n",
        "    \n",
        "    # Performance by occlusion level\n",
        "    occlusion_performance = analysis_df.groupby('occlusion_level').agg({\n",
        "        'absolute_error': ['mean', 'std'],\n",
        "        'exact_match': 'mean',\n",
        "        'confidence': 'mean',\n",
        "        'bias': 'mean'\n",
        "    }).round(3)\n",
        "    \n",
        "    print(\"\\nPerformance by Occlusion Level:\")\n",
        "    print(occlusion_performance)\n",
        "    \n",
        "    # Detailed analysis by model and occlusion\n",
        "    detailed_analysis = analysis_df.groupby(['model', 'occlusion_level']).agg({\n",
        "        'absolute_error': 'mean',\n",
        "        'exact_match': 'mean',\n",
        "        'bias': 'mean',\n",
        "        'confidence': 'mean'\n",
        "    }).round(3)\n",
        "    \n",
        "    print(\"\\nDetailed Analysis by Model and Occlusion:\")\n",
        "    print(detailed_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_visualizations"
      },
      "outputs": [],
      "source": [
        "if len(analysis_df) > 0:\n",
        "    # Create comprehensive visualizations\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    # 1. Accuracy by Occlusion Level\n",
        "    occlusion_accuracy = analysis_df.groupby(['occlusion_level', 'model'])['exact_match'].mean().unstack()\n",
        "    if not occlusion_accuracy.empty:\n",
        "        occlusion_accuracy.plot(kind='line', marker='o', ax=axes[0, 0])\n",
        "        axes[0, 0].set_title('Accuracy by Occlusion Level')\n",
        "        axes[0, 0].set_xlabel('Occlusion Level')\n",
        "        axes[0, 0].set_ylabel('Exact Match Accuracy')\n",
        "        axes[0, 0].legend(title='Model')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Average Error by Occlusion Level\n",
        "    occlusion_error = analysis_df.groupby(['occlusion_level', 'model'])['absolute_error'].mean().unstack()\n",
        "    if not occlusion_error.empty:\n",
        "        occlusion_error.plot(kind='line', marker='s', ax=axes[0, 1])\n",
        "        axes[0, 1].set_title('Average Error by Occlusion Level')\n",
        "        axes[0, 1].set_xlabel('Occlusion Level')\n",
        "        axes[0, 1].set_ylabel('Mean Absolute Error')\n",
        "        axes[0, 1].legend(title='Model')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Bias Analysis\n",
        "    bias_data = analysis_df.groupby('model')['bias'].mean()\n",
        "    colors = ['red' if x < 0 else 'green' for x in bias_data.values]\n",
        "    axes[0, 2].bar(bias_data.index, bias_data.values, color=colors, alpha=0.7)\n",
        "    axes[0, 2].set_title('Average Counting Bias by Model')\n",
        "    axes[0, 2].set_ylabel('Bias (Predicted - True)')\n",
        "    axes[0, 2].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Confidence vs Accuracy\n",
        "    for model in analysis_df['model'].unique():\n",
        "        model_data = analysis_df[analysis_df['model'] == model]\n",
        "        axes[1, 0].scatter(model_data['confidence'], model_data['exact_match'], \n",
        "                          label=model, alpha=0.6)\n",
        "    axes[1, 0].set_title('Confidence vs Accuracy')\n",
        "    axes[1, 0].set_xlabel('Confidence Score')\n",
        "    axes[1, 0].set_ylabel('Exact Match (1=correct, 0=incorrect)')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Error Distribution\n",
        "    analysis_df.boxplot(column='absolute_error', by='model', ax=axes[1, 1])\n",
        "    axes[1, 1].set_title('Error Distribution by Model')\n",
        "    axes[1, 1].set_ylabel('Absolute Error')\n",
        "    plt.sca(axes[1, 1])\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # 6. Performance Heatmap\n",
        "    if len(analysis_df['model'].unique()) > 1 and len(analysis_df['occlusion_level'].unique()) > 1:\n",
        "        pivot_data = analysis_df.pivot_table(values='exact_match', \n",
        "                                           index='model', \n",
        "                                           columns='occlusion_level', \n",
        "                                           aggfunc='mean')\n",
        "        im = axes[1, 2].imshow(pivot_data.values, cmap='RdYlGn', aspect='auto')\n",
        "        axes[1, 2].set_xticks(range(len(pivot_data.columns)))\n",
        "        axes[1, 2].set_xticklabels([f'{x:.0%}' for x in pivot_data.columns])\n",
        "        axes[1, 2].set_yticks(range(len(pivot_data.index)))\n",
        "        axes[1, 2].set_yticklabels(pivot_data.index)\n",
        "        axes[1, 2].set_title('Accuracy Heatmap: Model vs Occlusion')\n",
        "        axes[1, 2].set_xlabel('Occlusion Level')\n",
        "        axes[1, 2].set_ylabel('Model')\n",
        "        \n",
        "        # Add text annotations\n",
        "        for i in range(len(pivot_data.index)):\n",
        "            for j in range(len(pivot_data.columns)):\n",
        "                text = axes[1, 2].text(j, i, f'{pivot_data.iloc[i, j]:.2f}',\n",
        "                                     ha=\"center\", va=\"center\", color=\"black\")\n",
        "        \n",
        "        plt.colorbar(im, ax=axes[1, 2])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Interactive Plotly visualizations\n",
        "    print(\"\\nCreating interactive visualizations...\")\n",
        "    \n",
        "    # Interactive accuracy plot\n",
        "    fig_interactive = go.Figure()\n",
        "    \n",
        "    for model in analysis_df['model'].unique():\n",
        "        model_data = analysis_df[analysis_df['model'] == model]\n",
        "        occlusion_acc = model_data.groupby('occlusion_level')['exact_match'].mean()\n",
        "        \n",
        "        fig_interactive.add_trace(go.Scatter(\n",
        "            x=[f'{x:.0%}' for x in occlusion_acc.index],\n",
        "            y=occlusion_acc.values,\n",
        "            mode='lines+markers',\n",
        "            name=model,\n",
        "            line=dict(width=3),\n",
        "            marker=dict(size=8)\n",
        "        ))\n",
        "    \n",
        "    fig_interactive.update_layout(\n",
        "        title='Interactive: Model Accuracy vs Occlusion Level',\n",
        "        xaxis_title='Occlusion Level',\n",
        "        yaxis_title='Exact Match Accuracy',\n",
        "        hovermode='x unified'\n",
        "    )\n",
        "    \n",
        "    fig_interactive.show()\n",
        "    \n",
        "else:\n",
        "    print(\"Skipping visualizations due to insufficient data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_results"
      },
      "source": [
        "## Save Results and Generate Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_results"
      },
      "outputs": [],
      "source": [
        "# Save detailed results\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_filename = f\"synthetic_occlusion_results_{timestamp}.csv\"\n",
        "\n",
        "results_df.to_csv(results_filename, index=False)\n",
        "print(f\"Results saved to: {results_filename}\")\n",
        "\n",
        "# Generate summary report\n",
        "report = f\"\"\"\n",
        "# VLM Counting Bias Experiment - Synthetic Occlusion Results\n",
        "\n",
        "**Experiment Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Experiment Configuration\n",
        "- **Models Tested:** {', '.join(EXPERIMENT_CONFIG['models_to_test'])}\n",
        "- **Object Types:** {', '.join(EXPERIMENT_CONFIG['object_types'])}\n",
        "- **Object Counts:** {EXPERIMENT_CONFIG['object_counts']}\n",
        "- **Occlusion Levels:** {[f'{x:.0%}' for x in EXPERIMENT_CONFIG['occlusion_levels']]}\n",
        "- **Images per Condition:** {EXPERIMENT_CONFIG['num_images_per_condition']}\n",
        "- **Total Images Generated:** {len(synthetic_dataset)}\n",
        "- **Images Analyzed:** {len(results_df)}\n",
        "\n",
        "## Results Summary\n",
        "- **Total Predictions:** {len(results_df)}\n",
        "- **Successful Predictions:** {len(analysis_df)} ({len(analysis_df)/len(results_df):.1%})\n",
        "\"\"\"\n",
        "\n",
        "if len(analysis_df) > 0:\n",
        "    report += f\"\"\"\n",
        "- **Overall Accuracy:** {analysis_df['exact_match'].mean():.1%}\n",
        "- **Average Absolute Error:** {analysis_df['absolute_error'].mean():.2f}\n",
        "- **Average Confidence:** {analysis_df['confidence'].mean():.2f}\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "### Model Performance\n",
        "\"\"\"\n",
        "    \n",
        "    for model in analysis_df['model'].unique():\n",
        "        model_data = analysis_df[analysis_df['model'] == model]\n",
        "        report += f\"\"\"\n",
        "**{model}:**\n",
        "- Accuracy: {model_data['exact_match'].mean():.1%}\n",
        "- Avg Error: {model_data['absolute_error'].mean():.2f}\n",
        "- Avg Bias: {model_data['bias'].mean():.2f} ({'under-counting' if model_data['bias'].mean() < 0 else 'over-counting'})\n",
        "- Confidence: {model_data['confidence'].mean():.2f}\n",
        "\"\"\"\n",
        "    \n",
        "    report += \"\\n### Occlusion Impact\\n\"\n",
        "    for occlusion in sorted(analysis_df['occlusion_level'].unique()):\n",
        "        occ_data = analysis_df[analysis_df['occlusion_level'] == occlusion]\n",
        "        report += f\"\"\"\n",
        "**{occlusion:.0%} Occlusion:**\n",
        "- Accuracy: {occ_data['exact_match'].mean():.1%}\n",
        "- Avg Error: {occ_data['absolute_error'].mean():.2f}\n",
        "- Sample Size: {len(occ_data)}\n",
        "\"\"\"\n",
        "\n",
        "report += f\"\"\"\n",
        "\n",
        "## Files Generated\n",
        "- **Results CSV:** {results_filename}\n",
        "- **Synthetic Dataset:** {len(synthetic_dataset)} images in memory\n",
        "\n",
        "## Next Steps\n",
        "1. Run the real-world image evaluation notebook\n",
        "2. Compare results across synthetic and real scenarios\n",
        "3. Analyze failure cases and error patterns\n",
        "4. Consider additional models or prompting strategies\n",
        "\n",
        "---\n",
        "*Generated by VLM Counting Bias Research Platform*\n",
        "\"\"\"\n",
        "\n",
        "# Save report\n",
        "report_filename = f\"experiment_report_{timestamp}.md\"\n",
        "with open(report_filename, 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nExperiment report saved to: {report_filename}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(report)\n",
        "\n",
        "# Display download links in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"\\nDownload files:\")\n",
        "    files.download(results_filename)\n",
        "    files.download(report_filename)\n",
        "except:\n",
        "    print(f\"\\nFiles saved locally: {results_filename}, {report_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated a systematic evaluation of VLM counting capabilities under controlled synthetic occlusion conditions. \n",
        "\n",
        "### Key Insights:\n",
        "1. **Occlusion Impact**: Models show degraded performance as occlusion level increases\n",
        "2. **Model Differences**: Different VLMs exhibit varying robustness to occlusion\n",
        "3. **Bias Patterns**: Some models tend to under-count while others over-count\n",
        "4. **Confidence Calibration**: Model confidence may not correlate with actual accuracy\n",
        "\n",
        "### Next Steps:\n",
        "- Run the real-world image evaluation notebook (`02_counting_real_camouflage.ipynb`)\n",
        "- Compare synthetic vs. real-world performance\n",
        "- Investigate failure modes and potential improvements\n",
        "- Consider prompt engineering and few-shot learning approaches\n",
        "\n",
        "This research contributes to understanding the limitations and biases in current VLMs, informing better model development and deployment strategies."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
