{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "real_camouflage_title"
      },
      "source": [
        "# Vision-Language Model Counting: Real-World Images and Camouflage\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/vlm-counting-bias/blob/main/notebooks/02_counting_real_camouflage.ipynb)\n",
        "\n",
        "This notebook evaluates VLM counting performance on real-world images including challenging camouflage scenarios.\n",
        "\n",
        "## Objectives\n",
        "- Evaluate VLMs on real-world images from MS COCO\n",
        "- Test counting accuracy under natural occlusion conditions\n",
        "- Analyze performance on camouflaged objects\n",
        "- Compare real-world vs synthetic results\n",
        "\n",
        "## Setup Requirements\n",
        "- OpenAI API key for GPT-4V\n",
        "- HuggingFace token for model access\n",
        "- Internet connection for downloading COCO images\n",
        "- GPU recommended for local model inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install required packages in Colab\n",
        "!pip install openai transformers torch pillow opencv-python matplotlib pandas numpy plotly\n",
        "!pip install pycocotools requests tqdm\n",
        "!pip install accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import json\n",
        "import base64\n",
        "import io\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Dependencies loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_section"
      },
      "source": [
        "## Configuration and API Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api_setup"
      },
      "outputs": [],
      "source": [
        "# API Configuration\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
        "HF_TOKEN = os.getenv('HF_TOKEN', '')\n",
        "\n",
        "# If running in Colab, you can set keys directly (not recommended for production)\n",
        "if not OPENAI_API_KEY:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    except:\n",
        "        OPENAI_API_KEY = input(\"Enter OpenAI API Key: \")\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    try:\n",
        "        HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    except:\n",
        "        HF_TOKEN = input(\"Enter HuggingFace Token (optional): \") or None\n",
        "\n",
        "# Experiment configuration\n",
        "EXPERIMENT_CONFIG = {\n",
        "    'models_to_test': ['GPT-4V', 'BLIP-2'],  # Add 'LLaVA' if GPU available\n",
        "    'max_images_per_category': 5,  # Reduced for demo, increase for full experiment\n",
        "    'coco_categories': {\n",
        "        'person': 1,\n",
        "        'car': 3,\n",
        "        'bird': 16,\n",
        "        'cat': 17,\n",
        "        'dog': 18\n",
        "    },\n",
        "    'camouflage_scenarios': [\n",
        "        {\n",
        "            'description': 'Birds in trees',\n",
        "            'object_type': 'bird',\n",
        "            'expected_difficulty': 'high'\n",
        "        },\n",
        "        {\n",
        "            'description': 'Cats in shadows',\n",
        "            'object_type': 'cat',\n",
        "            'expected_difficulty': 'medium'\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "for key, value in EXPERIMENT_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading"
      },
      "source": [
        "## Real-World Data Loading\n",
        "\n",
        "We'll use a curated set of real-world images with known object counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loader"
      },
      "outputs": [],
      "source": [
        "class RealWorldDataLoader:\n",
        "    def __init__(self):\n",
        "        self.coco_base_url = \"http://images.cocodataset.org/val2017/\"\n",
        "        \n",
        "        # Curated dataset with known counts (manually verified)\n",
        "        self.curated_images = {\n",
        "            # Format: filename -> {object_type, count, difficulty, description}\n",
        "            \"000000000139.jpg\": {\n",
        "                \"object_type\": \"person\",\n",
        "                \"count\": 4,\n",
        "                \"difficulty\": \"medium\",\n",
        "                \"description\": \"Group of people on tennis court\",\n",
        "                \"occlusion_level\": \"partial\"\n",
        "            },\n",
        "            \"000000000285.jpg\": {\n",
        "                \"object_type\": \"person\",\n",
        "                \"count\": 3,\n",
        "                \"difficulty\": \"easy\",\n",
        "                \"description\": \"People skiing\",\n",
        "                \"occlusion_level\": \"minimal\"\n",
        "            },\n",
        "            \"000000000632.jpg\": {\n",
        "                \"object_type\": \"person\",\n",
        "                \"count\": 2,\n",
        "                \"difficulty\": \"hard\",\n",
        "                \"description\": \"Surfers on beach, partially visible\",\n",
        "                \"occlusion_level\": \"high\"\n",
        "            },\n",
        "            \"000000000724.jpg\": {\n",
        "                \"object_type\": \"bird\",\n",
        "                \"count\": 3,\n",
        "                \"difficulty\": \"hard\",\n",
        "                \"description\": \"Birds on water, some camouflaged\",\n",
        "                \"occlusion_level\": \"camouflage\"\n",
        "            },\n",
        "            \"000000001000.jpg\": {\n",
        "                \"object_type\": \"car\",\n",
        "                \"count\": 5,\n",
        "                \"difficulty\": \"medium\",\n",
        "                \"description\": \"Cars in parking area\",\n",
        "                \"occlusion_level\": \"partial\"\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Additional challenging camouflage scenarios\n",
        "        self.camouflage_urls = [\n",
        "            {\n",
        "                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Camouflaged_gecko.jpg/512px-Camouflaged_gecko.jpg\",\n",
        "                \"object_type\": \"gecko\",\n",
        "                \"count\": 1,\n",
        "                \"difficulty\": \"extreme\",\n",
        "                \"description\": \"Gecko camouflaged on tree bark\",\n",
        "                \"occlusion_level\": \"camouflage\"\n",
        "            },\n",
        "            {\n",
        "                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/Leaf_insect.jpg/512px-Leaf_insect.jpg\",\n",
        "                \"object_type\": \"insect\",\n",
        "                \"count\": 1,\n",
        "                \"difficulty\": \"extreme\",\n",
        "                \"description\": \"Leaf insect camouflaged among leaves\",\n",
        "                \"occlusion_level\": \"camouflage\"\n",
        "            }\n",
        "        ]\n",
        "    \n",
        "    def download_image(self, url, timeout=10):\n",
        "        \"\"\"Download image from URL\"\"\"\n",
        "        try:\n",
        "            response = requests.get(url, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "            return Image.open(io.BytesIO(response.content))\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {url}: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def load_curated_dataset(self):\n",
        "        \"\"\"Load curated real-world images\"\"\"\n",
        "        dataset = []\n",
        "        \n",
        "        print(\"Loading curated COCO images...\")\n",
        "        for filename, metadata in tqdm(self.curated_images.items()):\n",
        "            url = self.coco_base_url + filename\n",
        "            image = self.download_image(url)\n",
        "            \n",
        "            if image is not None:\n",
        "                # Resize if too large\n",
        "                if max(image.size) > 1024:\n",
        "                    ratio = 1024 / max(image.size)\n",
        "                    new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
        "                    image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
        "                \n",
        "                dataset_entry = {\n",
        "                    'image_id': filename.replace('.jpg', ''),\n",
        "                    'image': image,\n",
        "                    'source': 'coco',\n",
        "                    'url': url,\n",
        "                    **metadata\n",
        "                }\n",
        "                dataset.append(dataset_entry)\n",
        "            else:\n",
        "                print(f\"Failed to load {filename}\")\n",
        "        \n",
        "        print(f\"\\nLoading camouflage images...\")\n",
        "        for idx, cam_data in enumerate(tqdm(self.camouflage_urls)):\n",
        "            image = self.download_image(cam_data['url'])\n",
        "            \n",
        "            if image is not None:\n",
        "                # Resize if too large\n",
        "                if max(image.size) > 1024:\n",
        "                    ratio = 1024 / max(image.size)\n",
        "                    new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
        "                    image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
        "                \n",
        "                dataset_entry = {\n",
        "                    'image_id': f'camouflage_{idx+1}',\n",
        "                    'image': image,\n",
        "                    'source': 'camouflage',\n",
        "                    'url': cam_data['url'],\n",
        "                    'object_type': cam_data['object_type'],\n",
        "                    'count': cam_data['count'],\n",
        "                    'difficulty': cam_data['difficulty'],\n",
        "                    'description': cam_data['description'],\n",
        "                    'occlusion_level': cam_data['occlusion_level']\n",
        "                }\n",
        "                dataset.append(dataset_entry)\n",
        "        \n",
        "        return dataset\n",
        "    \n",
        "    def create_synthetic_challenging_images(self):\n",
        "        \"\"\"Create some challenging synthetic scenarios\"\"\"\n",
        "        synthetic_images = []\n",
        "        \n",
        "        # This would typically load from a more sophisticated generator\n",
        "        # For now, we'll create some basic challenging scenarios\n",
        "        \n",
        "        return synthetic_images\n",
        "\n",
        "# Initialize data loader\n",
        "data_loader = RealWorldDataLoader()\n",
        "print(\"Real-world data loader initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_dataset"
      },
      "outputs": [],
      "source": [
        "# Load the real-world dataset\n",
        "print(\"Loading real-world dataset...\")\n",
        "real_world_dataset = data_loader.load_curated_dataset()\n",
        "\n",
        "print(f\"\\nLoaded {len(real_world_dataset)} real-world images\")\n",
        "\n",
        "# Display sample images\n",
        "if len(real_world_dataset) > 0:\n",
        "    n_display = min(len(real_world_dataset), 6)\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i in range(n_display):\n",
        "        sample = real_world_dataset[i]\n",
        "        axes[i].imshow(sample['image'])\n",
        "        title = f\"{sample['image_id']}\\n{sample['object_type'].title()}: {sample['count']}\\nDifficulty: {sample['difficulty']}\"\n",
        "        axes[i].set_title(title)\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(n_display, len(axes)):\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Sample Real-World Images', y=1.02, fontsize=16)\n",
        "    plt.show()\n",
        "    \n",
        "    # Show dataset statistics\n",
        "    df_stats = pd.DataFrame(real_world_dataset)\n",
        "    print(\"\\nDataset Statistics:\")\n",
        "    print(f\"Object types: {df_stats['object_type'].value_counts().to_dict()}\")\n",
        "    print(f\"Difficulty levels: {df_stats['difficulty'].value_counts().to_dict()}\")\n",
        "    print(f\"Occlusion types: {df_stats['occlusion_level'].value_counts().to_dict()}\")\n",
        "    print(f\"Count distribution: {df_stats['count'].value_counts().sort_index().to_dict()}\")\n",
        "else:\n",
        "    print(\"No images loaded successfully. Please check internet connection and URLs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlm_interface"
      },
      "source": [
        "## VLM Interface (Same as Synthetic Notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlm_implementation"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import requests\n",
        "import re\n",
        "\n",
        "class VLMInterface:\n",
        "    def __init__(self, openai_key=None, hf_token=None):\n",
        "        self.openai_key = openai_key\n",
        "        self.hf_token = hf_token\n",
        "        \n",
        "        if openai_key:\n",
        "            self.openai_client = openai.OpenAI(api_key=openai_key)\n",
        "    \n",
        "    def count_objects_gpt4v(self, image_base64, object_type, max_retries=3):\n",
        "        \"\"\"Count objects using GPT-4V\"\"\"\n",
        "        if not self.openai_key:\n",
        "            raise ValueError(\"OpenAI API key required for GPT-4V\")\n",
        "        \n",
        "        # Enhanced prompt for real-world images\n",
        "        prompt = f\"\"\"Look at this image very carefully and count the number of {object_type} visible.\n",
        "        \n",
        "        Pay special attention to:\n",
        "        - Objects that might be partially hidden or occluded\n",
        "        - Objects that blend into the background (camouflaged)\n",
        "        - Small or distant objects that might be hard to see\n",
        "        - Only count objects that are clearly identifiable as {object_type}\n",
        "        \n",
        "        Please respond with ONLY a JSON object in this format:\n",
        "        {{\n",
        "            \"count\": <number>,\n",
        "            \"confidence\": <0.0 to 1.0>,\n",
        "            \"reasoning\": \"<detailed explanation of what you see and why you chose this count>\",\n",
        "            \"visible_objects\": [\"brief description of each object you counted\"]\n",
        "        }}\n",
        "        \n",
        "        Be precise and honest about your confidence level.\"\"\"\n",
        "        \n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                # the newest OpenAI model is \"gpt-4o\" which was released May 13, 2024.\n",
        "                # do not change this unless explicitly requested by the user\n",
        "                response = self.openai_client.chat.completions.create(\n",
        "                    model=\"gpt-4o\",\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": [\n",
        "                                {\"type\": \"text\", \"text\": prompt},\n",
        "                                {\n",
        "                                    \"type\": \"image_url\",\n",
        "                                    \"image_url\": {\n",
        "                                        \"url\": f\"data:image/png;base64,{image_base64}\"\n",
        "                                    }\n",
        "                                }\n",
        "                            ]\n",
        "                        }\n",
        "                    ],\n",
        "                    max_tokens=300,\n",
        "                    response_format={\"type\": \"json_object\"}\n",
        "                )\n",
        "                \n",
        "                result_text = response.choices[0].message.content\n",
        "                result = json.loads(result_text)\n",
        "                \n",
        "                return {\n",
        "                    'count': int(result.get('count', 0)),\n",
        "                    'confidence': float(result.get('confidence', 0.5)),\n",
        "                    'reasoning': result.get('reasoning', ''),\n",
        "                    'visible_objects': result.get('visible_objects', []),\n",
        "                    'raw_response': result_text\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    return {\n",
        "                        'count': 0,\n",
        "                        'confidence': 0.0,\n",
        "                        'error': str(e),\n",
        "                        'reasoning': f'Error after {max_retries} attempts'\n",
        "                    }\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "    \n",
        "    def count_objects_blip2(self, image_base64, object_type, max_retries=3):\n",
        "        \"\"\"Count objects using BLIP-2 via HuggingFace Inference API\"\"\"\n",
        "        \n",
        "        # Using HuggingFace Inference API for BLIP-2\n",
        "        api_url = \"https://api-inference.huggingface.co/models/Salesforce/blip2-opt-2.7b\"\n",
        "        \n",
        "        headers = {}\n",
        "        if self.hf_token:\n",
        "            headers[\"Authorization\"] = f\"Bearer {self.hf_token}\"\n",
        "        \n",
        "        # Convert base64 to bytes\n",
        "        image_bytes = base64.b64decode(image_base64)\n",
        "        \n",
        "        # Enhanced question for real-world scenarios\n",
        "        question = f\"How many {object_type} can you see in this image? Look carefully for partially hidden or camouflaged ones.\"\n",
        "        \n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = requests.post(\n",
        "                    api_url,\n",
        "                    headers=headers,\n",
        "                    data=image_bytes,\n",
        "                    params={\"question\": question}\n",
        "                )\n",
        "                \n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    answer = result[0]['answer'] if isinstance(result, list) else result.get('answer', '')\n",
        "                    \n",
        "                    # Extract number from answer\n",
        "                    count = self.extract_number_from_text(answer)\n",
        "                    \n",
        "                    # Enhanced confidence estimation\n",
        "                    confidence = 0.8 if any(str(i) in answer for i in range(20)) else 0.5\n",
        "                    if 'unsure' in answer.lower() or 'maybe' in answer.lower():\n",
        "                        confidence *= 0.7\n",
        "                    \n",
        "                    return {\n",
        "                        'count': count,\n",
        "                        'confidence': confidence,\n",
        "                        'reasoning': answer,\n",
        "                        'raw_response': str(result)\n",
        "                    }\n",
        "                else:\n",
        "                    if attempt == max_retries - 1:\n",
        "                        return {\n",
        "                            'count': 0,\n",
        "                            'confidence': 0.0,\n",
        "                            'error': f'HTTP {response.status_code}: {response.text}',\n",
        "                            'reasoning': 'API request failed'\n",
        "                        }\n",
        "                    \n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    return {\n",
        "                        'count': 0,\n",
        "                        'confidence': 0.0,\n",
        "                        'error': str(e),\n",
        "                        'reasoning': f'Error after {max_retries} attempts'\n",
        "                    }\n",
        "                \n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "    \n",
        "    def extract_number_from_text(self, text):\n",
        "        \"\"\"Extract a number from text response\"\"\"\n",
        "        if not text:\n",
        "            return 0\n",
        "        \n",
        "        # Look for explicit numbers\n",
        "        numbers = re.findall(r'\\b\\d+\\b', text)\n",
        "        if numbers:\n",
        "            # Take the first number, but prefer numbers that appear with counting words\n",
        "            for num_str in numbers:\n",
        "                num = int(num_str)\n",
        "                if num < 50:  # Reasonable upper bound for object counting\n",
        "                    return num\n",
        "            return int(numbers[0])\n",
        "        \n",
        "        # Look for written numbers\n",
        "        number_words = {\n",
        "            'zero': 0, 'none': 0, 'no': 0,\n",
        "            'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,\n",
        "            'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10,\n",
        "            'eleven': 11, 'twelve': 12, 'thirteen': 13, 'fourteen': 14, 'fifteen': 15\n",
        "        }\n",
        "        \n",
        "        text_lower = text.lower()\n",
        "        for word, num in number_words.items():\n",
        "            if word in text_lower:\n",
        "                return num\n",
        "        \n",
        "        # Default to 0 if no number found\n",
        "        return 0\n",
        "    \n",
        "    def count_objects(self, model_name, image_base64, object_type):\n",
        "        \"\"\"Main interface for counting objects with different models\"\"\"\n",
        "        if model_name == 'GPT-4V':\n",
        "            return self.count_objects_gpt4v(image_base64, object_type)\n",
        "        elif model_name == 'BLIP-2':\n",
        "            return self.count_objects_blip2(image_base64, object_type)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "# Initialize VLM interface\n",
        "vlm_interface = VLMInterface(openai_key=OPENAI_API_KEY, hf_token=HF_TOKEN)\n",
        "print(\"VLM interface initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experiment_execution"
      },
      "source": [
        "## Experiment Execution on Real-World Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_real_experiment"
      },
      "outputs": [],
      "source": [
        "def run_real_world_experiment(dataset, models_to_test, vlm_interface):\n",
        "    \"\"\"Run counting experiment on real-world dataset\"\"\"\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(f\"Running real-world experiment on {len(dataset)} images with {len(models_to_test)} models...\")\n",
        "    \n",
        "    progress_bar = tqdm(total=len(dataset) * len(models_to_test))\n",
        "    \n",
        "    for img_data in dataset:\n",
        "        # Convert image to base64\n",
        "        img_buffer = io.BytesIO()\n",
        "        img_data['image'].save(img_buffer, format='PNG')\n",
        "        img_base64 = base64.b64encode(img_buffer.getvalue()).decode()\n",
        "        \n",
        "        for model_name in models_to_test:\n",
        "            progress_bar.set_description(f\"Processing {img_data['image_id']} with {model_name}\")\n",
        "            \n",
        "            try:\n",
        "                # Get prediction from model\n",
        "                prediction = vlm_interface.count_objects(\n",
        "                    model_name, img_base64, img_data['object_type']\n",
        "                )\n",
        "                \n",
        "                # Store result\n",
        "                result = {\n",
        "                    'image_id': img_data['image_id'],\n",
        "                    'model': model_name,\n",
        "                    'true_count': img_data['count'],\n",
        "                    'predicted_count': prediction['count'],\n",
        "                    'confidence': prediction['confidence'],\n",
        "                    'object_type': img_data['object_type'],\n",
        "                    'difficulty': img_data['difficulty'],\n",
        "                    'occlusion_level': img_data['occlusion_level'],\n",
        "                    'source': img_data['source'],\n",
        "                    'description': img_data['description'],\n",
        "                    'error': prediction.get('error'),\n",
        "                    'reasoning': prediction.get('reasoning', ''),\n",
        "                    'visible_objects': prediction.get('visible_objects', []),\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                \n",
        "                # Calculate metrics\n",
        "                if not prediction.get('error'):\n",
        "                    result['absolute_error'] = abs(prediction['count'] - img_data['count'])\n",
        "                    result['relative_error'] = result['absolute_error'] / max(img_data['count'], 1)\n",
        "                    result['bias'] = prediction['count'] - img_data['count']\n",
        "                    result['exact_match'] = prediction['count'] == img_data['count']\n",
        "                    \n",
        "                    # Categorize error types\n",
        "                    if result['bias'] > 0:\n",
        "                        result['error_type'] = 'over_count'\n",
        "                    elif result['bias'] < 0:\n",
        "                        result['error_type'] = 'under_count'\n",
        "                    else:\n",
        "                        result['error_type'] = 'exact'\n",
        "                \n",
        "                results.append(result)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"\\nError processing {img_data['image_id']} with {model_name}: {str(e)}\")\n",
        "                \n",
        "                # Store error result\n",
        "                result = {\n",
        "                    'image_id': img_data['image_id'],\n",
        "                    'model': model_name,\n",
        "                    'true_count': img_data['count'],\n",
        "                    'predicted_count': 0,\n",
        "                    'confidence': 0.0,\n",
        "                    'object_type': img_data['object_type'],\n",
        "                    'difficulty': img_data['difficulty'],\n",
        "                    'occlusion_level': img_data['occlusion_level'],\n",
        "                    'source': img_data['source'],\n",
        "                    'description': img_data['description'],\n",
        "                    'error': str(e),\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                results.append(result)\n",
        "            \n",
        "            progress_bar.update(1)\n",
        "            \n",
        "            # Rate limiting delay\n",
        "            time.sleep(1.5)  # Slightly longer delay for API stability\n",
        "    \n",
        "    progress_bar.close()\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run the experiment if we have data\n",
        "if len(real_world_dataset) > 0:\n",
        "    print(\"Starting real-world experiment...\")\n",
        "    \n",
        "    real_results_df = run_real_world_experiment(\n",
        "        dataset=real_world_dataset,\n",
        "        models_to_test=EXPERIMENT_CONFIG['models_to_test'],\n",
        "        vlm_interface=vlm_interface\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nReal-world experiment completed! Collected {len(real_results_df)} results.\")\n",
        "    \n",
        "    # Display summary\n",
        "    print(\"\\nSummary Statistics:\")\n",
        "    successful_results = real_results_df[real_results_df['error'].isna()]\n",
        "    print(f\"Successful predictions: {len(successful_results)}/{len(real_results_df)} ({len(successful_results)/len(real_results_df):.1%})\")\n",
        "    \n",
        "    if len(successful_results) > 0:\n",
        "        print(f\"Average absolute error: {successful_results['absolute_error'].mean():.2f}\")\n",
        "        print(f\"Exact match accuracy: {successful_results['exact_match'].mean():.1%}\")\n",
        "        print(f\"Average confidence: {successful_results['confidence'].mean():.2f}\")\n",
        "        print(f\"Over-counting bias: {successful_results[successful_results['bias'] > 0]['bias'].mean():.2f}\")\n",
        "        print(f\"Under-counting bias: {successful_results[successful_results['bias'] < 0]['bias'].mean():.2f}\")\n",
        "else:\n",
        "    print(\"No real-world data available for experiment. Please check data loading.\")\n",
        "    real_results_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "detailed_analysis"
      },
      "source": [
        "## Detailed Analysis of Real-World Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analyze_real_results"
      },
      "outputs": [],
      "source": [
        "if len(real_results_df) > 0 and len(real_results_df[real_results_df['error'].isna()]) > 0:\n",
        "    # Filter successful results for analysis\n",
        "    analysis_df = real_results_df[real_results_df['error'].isna()].copy()\n",
        "    \n",
        "    print(f\"Analyzing {len(analysis_df)} successful predictions...\")\n",
        "    \n",
        "    # Performance by difficulty level\n",
        "    print(\"\\n=== Performance by Difficulty Level ===\")\n",
        "    difficulty_performance = analysis_df.groupby(['difficulty', 'model']).agg({\n",
        "        'absolute_error': ['mean', 'std'],\n",
        "        'exact_match': 'mean',\n",
        "        'confidence': 'mean',\n",
        "        'bias': 'mean'\n",
        "    }).round(3)\n",
        "    \n",
        "    print(difficulty_performance)\n",
        "    \n",
        "    # Performance by occlusion type\n",
        "    print(\"\\n=== Performance by Occlusion Type ===\")\n",
        "    occlusion_performance = analysis_df.groupby(['occlusion_level', 'model']).agg({\n",
        "        'absolute_error': ['mean', 'std'],\n",
        "        'exact_match': 'mean',\n",
        "        'confidence': 'mean',\n",
        "        'bias': 'mean'\n",
        "    }).round(3)\n",
        "    \n",
        "    print(occlusion_performance)\n",
        "    \n",
        "    # Camouflage-specific analysis\n",
        "    camouflage_data = analysis_df[analysis_df['occlusion_level'] == 'camouflage']\n",
        "    if len(camouflage_data) > 0:\n",
        "        print(\"\\n=== Camouflage Performance ===\")\n",
        "        camouflage_performance = camouflage_data.groupby('model').agg({\n",
        "            'absolute_error': 'mean',\n",
        "            'exact_match': 'mean',\n",
        "            'confidence': 'mean',\n",
        "            'bias': 'mean'\n",
        "        }).round(3)\n",
        "        \n",
        "        print(camouflage_performance)\n",
        "        \n",
        "        print(\"\\nCamouflage Cases Analysis:\")\n",
        "        for _, row in camouflage_data.iterrows():\n",
        "            status = \"✓ Correct\" if row['exact_match'] else \"✗ Incorrect\"\n",
        "            print(f\"{status} | {row['model']}: {row['predicted_count']} vs {row['true_count']} | {row['description']}\")\n",
        "    \n",
        "    # Error type analysis\n",
        "    print(\"\\n=== Error Type Distribution ===\")\n",
        "    error_dist = analysis_df.groupby(['model', 'error_type']).size().unstack(fill_value=0)\n",
        "    error_dist_pct = error_dist.div(error_dist.sum(axis=1), axis=0) * 100\n",
        "    print(\"Counts:\")\n",
        "    print(error_dist)\n",
        "    print(\"\\nPercentages:\")\n",
        "    print(error_dist_pct.round(1))\n",
        "    \n",
        "    # Detailed case analysis\n",
        "    print(\"\\n=== Detailed Case Analysis ===\")\n",
        "    for _, row in analysis_df.iterrows():\n",
        "        emoji = \"✓\" if row['exact_match'] else \"✗\"\n",
        "        conf_str = f\"({row['confidence']:.2f})\"\n",
        "        print(f\"{emoji} {row['image_id']} | {row['model']} | Pred: {row['predicted_count']}, True: {row['true_count']} {conf_str} | {row['description']}\")\n",
        "        if not row['exact_match'] and row.get('reasoning'):\n",
        "            print(f\"    Reasoning: {row['reasoning'][:200]}{'...' if len(row['reasoning']) > 200 else ''}\")\n",
        "\n",
        "else:\n",
        "    print(\"No successful results to analyze. Please check API configurations and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualizations"
      },
      "source": [
        "## Advanced Visualizations for Real-World Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_real_visualizations"
      },
      "outputs": [],
      "source": [
        "if len(real_results_df) > 0 and len(real_results_df[real_results_df['error'].isna()]) > 0:\n",
        "    analysis_df = real_results_df[real_results_df['error'].isna()].copy()\n",
        "    \n",
        "    # Create comprehensive visualizations\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "    \n",
        "    # 1. Accuracy by Difficulty Level\n",
        "    if len(analysis_df['difficulty'].unique()) > 1:\n",
        "        difficulty_accuracy = analysis_df.groupby(['difficulty', 'model'])['exact_match'].mean().unstack(fill_value=0)\n",
        "        if not difficulty_accuracy.empty:\n",
        "            difficulty_accuracy.plot(kind='bar', ax=axes[0, 0], width=0.8)\n",
        "            axes[0, 0].set_title('Accuracy by Difficulty Level')\n",
        "            axes[0, 0].set_xlabel('Difficulty Level')\n",
        "            axes[0, 0].set_ylabel('Exact Match Accuracy')\n",
        "            axes[0, 0].legend(title='Model')\n",
        "            axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "            axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Performance by Occlusion Type\n",
        "    if len(analysis_df['occlusion_level'].unique()) > 1:\n",
        "        occlusion_accuracy = analysis_df.groupby(['occlusion_level', 'model'])['exact_match'].mean().unstack(fill_value=0)\n",
        "        if not occlusion_accuracy.empty:\n",
        "            occlusion_accuracy.plot(kind='bar', ax=axes[0, 1], width=0.8)\n",
        "            axes[0, 1].set_title('Accuracy by Occlusion Type')\n",
        "            axes[0, 1].set_xlabel('Occlusion Level')\n",
        "            axes[0, 1].set_ylabel('Exact Match Accuracy')\n",
        "            axes[0, 1].legend(title='Model')\n",
        "            axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Error Distribution by Model\n",
        "    if 'error_type' in analysis_df.columns:\n",
        "        error_counts = analysis_df.groupby(['model', 'error_type']).size().unstack(fill_value=0)\n",
        "        if not error_counts.empty:\n",
        "            error_counts.plot(kind='bar', stacked=True, ax=axes[1, 0])\n",
        "            axes[1, 0].set_title('Error Type Distribution by Model')\n",
        "            axes[1, 0].set_xlabel('Model')\n",
        "            axes[1, 0].set_ylabel('Count')\n",
        "            axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Confidence vs Accuracy Scatter\n",
        "    for model in analysis_df['model'].unique():\n",
        "        model_data = analysis_df[analysis_df['model'] == model]\n",
        "        scatter = axes[1, 1].scatter(model_data['confidence'], model_data['exact_match'], \n",
        "                                   label=model, alpha=0.7, s=60)\n",
        "    \n",
        "    axes[1, 1].set_title('Confidence vs Accuracy by Model')\n",
        "    axes[1, 1].set_xlabel('Model Confidence')\n",
        "    axes[1, 1].set_ylabel('Exact Match (1=correct, 0=incorrect)')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Bias by Object Type\n",
        "    if len(analysis_df['object_type'].unique()) > 1:\n",
        "        bias_by_object = analysis_df.groupby(['object_type', 'model'])['bias'].mean().unstack(fill_value=0)\n",
        "        if not bias_by_object.empty:\n",
        "            bias_by_object.plot(kind='bar', ax=axes[2, 0])\n",
        "            axes[2, 0].set_title('Average Bias by Object Type')\n",
        "            axes[2, 0].set_xlabel('Object Type')\n",
        "            axes[2, 0].set_ylabel('Bias (Predicted - True)')\n",
        "            axes[2, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "            axes[2, 0].tick_params(axis='x', rotation=45)\n",
        "            axes[2, 0].grid(True, alpha=0.3)\n",
        "            axes[2, 0].legend(title='Model')\n",
        "    \n",
        "    # 6. Confidence Distribution by Source\n",
        "    if len(analysis_df['source'].unique()) > 1:\n",
        "        for source in analysis_df['source'].unique():\n",
        "            source_data = analysis_df[analysis_df['source'] == source]\n",
        "            axes[2, 1].hist(source_data['confidence'], alpha=0.6, label=f'{source.title()} Images', bins=10)\n",
        "        \n",
        "        axes[2, 1].set_title('Confidence Distribution by Image Source')\n",
        "        axes[2, 1].set_xlabel('Model Confidence')\n",
        "        axes[2, 1].set_ylabel('Frequency')\n",
        "        axes[2, 1].legend()\n",
        "        axes[2, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Interactive Plotly visualization for detailed exploration\n",
        "    print(\"\\nCreating interactive visualization...\")\n",
        "    \n",
        "    fig_interactive = px.scatter(\n",
        "        analysis_df,\n",
        "        x='confidence',\n",
        "        y='absolute_error',\n",
        "        color='model',\n",
        "        size='true_count',\n",
        "        hover_data=['image_id', 'object_type', 'difficulty', 'description'],\n",
        "        title='Interactive: Confidence vs Error Analysis'\n",
        "    )\n",
        "    \n",
        "    fig_interactive.update_layout(\n",
        "        xaxis_title='Model Confidence',\n",
        "        yaxis_title='Absolute Error',\n",
        "        hovermode='closest'\n",
        "    )\n",
        "    \n",
        "    fig_interactive.show()\n",
        "    \n",
        "    # Camouflage-specific visualization if available\n",
        "    camouflage_data = analysis_df[analysis_df['occlusion_level'] == 'camouflage']\n",
        "    if len(camouflage_data) > 0:\n",
        "        print(\"\\nCamouflage Performance Visualization:\")\n",
        "        \n",
        "        fig_cam = px.bar(\n",
        "            camouflage_data,\n",
        "            x='image_id',\n",
        "            y='predicted_count',\n",
        "            color='model',\n",
        "            title='Camouflage Object Counting Performance',\n",
        "            hover_data=['true_count', 'confidence', 'description']\n",
        "        )\n",
        "        \n",
        "        # Add ground truth line\n",
        "        for i, (_, row) in enumerate(camouflage_data.iterrows()):\n",
        "            fig_cam.add_hline(\n",
        "                y=row['true_count'],\n",
        "                line_dash=\"dash\",\n",
        "                line_color=\"red\",\n",
        "                annotation_text=f\"True: {row['true_count']}\"\n",
        "            )\n",
        "        \n",
        "        fig_cam.show()\n",
        "\n",
        "else:\n",
        "    print(\"Insufficient data for visualizations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparative_analysis"
      },
      "source": [
        "## Comparative Analysis: Synthetic vs Real-World"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compare_results"
      },
      "outputs": [],
      "source": [
        "# This section would compare results from the synthetic notebook\n",
        "# For demonstration, we'll create a framework for comparison\n",
        "\n",
        "def create_comparison_analysis(real_results, synthetic_results=None):\n",
        "    \"\"\"Compare real-world vs synthetic results if available\"\"\"\n",
        "    \n",
        "    print(\"=== Real-World vs Synthetic Comparison ===\")\n",
        "    \n",
        "    if synthetic_results is None:\n",
        "        print(\"Note: Run the synthetic occlusion notebook first to enable full comparison.\")\n",
        "        print(\"For now, analyzing real-world results only.\\n\")\n",
        "        \n",
        "        if len(real_results) > 0:\n",
        "            real_analysis = real_results[real_results['error'].isna()]\n",
        "            \n",
        "            print(\"Real-World Results Summary:\")\n",
        "            print(f\"- Total successful predictions: {len(real_analysis)}\")\n",
        "            print(f\"- Average accuracy: {real_analysis['exact_match'].mean():.1%}\")\n",
        "            print(f\"- Average confidence: {real_analysis['confidence'].mean():.3f}\")\n",
        "            print(f\"- Average absolute error: {real_analysis['absolute_error'].mean():.2f}\")\n",
        "            \n",
        "            # Breakdown by challenge type\n",
        "            print(\"\\nPerformance by Challenge Type:\")\n",
        "            challenge_performance = real_analysis.groupby('occlusion_level').agg({\n",
        "                'exact_match': 'mean',\n",
        "                'confidence': 'mean',\n",
        "                'absolute_error': 'mean'\n",
        "            }).round(3)\n",
        "            \n",
        "            for challenge_type, metrics in challenge_performance.iterrows():\n",
        "                print(f\"  {challenge_type.title()}:\")\n",
        "                print(f\"    - Accuracy: {metrics['exact_match']:.1%}\")\n",
        "                print(f\"    - Confidence: {metrics['confidence']:.3f}\")\n",
        "                print(f\"    - Avg Error: {metrics['absolute_error']:.2f}\")\n",
        "    \n",
        "    else:\n",
        "        # Full comparison would go here\n",
        "        print(\"Full synthetic vs real-world comparison available!\")\n",
        "        # Implementation would compare performance patterns, bias differences, etc.\n",
        "        pass\n",
        "\n",
        "# Run comparison analysis\n",
        "if len(real_results_df) > 0:\n",
        "    create_comparison_analysis(real_results_df)\n",
        "else:\n",
        "    print(\"No real-world results available for comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_results"
      },
      "source": [
        "## Save Results and Generate Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_real_results"
      },
      "outputs": [],
      "source": [
        "if len(real_results_df) > 0:\n",
        "    # Save detailed results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_filename = f\"real_world_camouflage_results_{timestamp}.csv\"\n",
        "    \n",
        "    real_results_df.to_csv(results_filename, index=False)\n",
        "    print(f\"Results saved to: {results_filename}\")\n",
        "    \n",
        "    # Generate comprehensive report\n",
        "    successful_results = real_results_df[real_results_df['error'].isna()]\n",
        "    \n",
        "    report = f\"\"\"\n",
        "# VLM Counting Bias Experiment - Real-World and Camouflage Results\n",
        "\n",
        "**Experiment Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Experiment Configuration\n",
        "- **Models Tested:** {', '.join(EXPERIMENT_CONFIG['models_to_test'])}\n",
        "- **Total Images Analyzed:** {len(real_results_df)}\n",
        "- **Successful Predictions:** {len(successful_results)} ({len(successful_results)/len(real_results_df):.1%})\n",
        "- **Image Sources:** COCO validation set, Wikimedia camouflage examples\n",
        "\n",
        "## Dataset Composition\n",
        "\"\"\"\n",
        "    \n",
        "    if len(successful_results) > 0:\n",
        "        # Dataset statistics\n",
        "        dataset_stats = successful_results.groupby(['source', 'object_type']).size().to_dict()\n",
        "        report += \"\\n### Images by Source and Object Type\\n\"\n",
        "        for (source, obj_type), count in dataset_stats.items():\n",
        "            report += f\"- {source.title()} {obj_type}: {count} images\\n\"\n",
        "        \n",
        "        # Difficulty distribution\n",
        "        difficulty_dist = successful_results['difficulty'].value_counts()\n",
        "        report += f\"\\n### Difficulty Distribution\\n\"\n",
        "        for difficulty, count in difficulty_dist.items():\n",
        "            report += f\"- {difficulty.title()}: {count} images\\n\"\n",
        "        \n",
        "        report += f\"\"\"\n",
        "\n",
        "## Overall Results\n",
        "- **Overall Accuracy:** {successful_results['exact_match'].mean():.1%}\n",
        "- **Average Absolute Error:** {successful_results['absolute_error'].mean():.2f}\n",
        "- **Average Confidence:** {successful_results['confidence'].mean():.3f}\n",
        "- **Over-counting Rate:** {(successful_results['bias'] > 0).mean():.1%}\n",
        "- **Under-counting Rate:** {(successful_results['bias'] < 0).mean():.1%}\n",
        "\n",
        "## Model Performance Comparison\n",
        "\"\"\"\n",
        "        \n",
        "        for model in successful_results['model'].unique():\n",
        "            model_data = successful_results[successful_results['model'] == model]\n",
        "            report += f\"\"\"\n",
        "### {model}\n",
        "- **Accuracy:** {model_data['exact_match'].mean():.1%}\n",
        "- **Avg Error:** {model_data['absolute_error'].mean():.2f}\n",
        "- **Avg Bias:** {model_data['bias'].mean():.2f} ({'under-counting' if model_data['bias'].mean() < 0 else 'over-counting' if model_data['bias'].mean() > 0 else 'balanced'})\n",
        "- **Confidence:** {model_data['confidence'].mean():.3f}\n",
        "- **Total Predictions:** {len(model_data)}\n",
        "\"\"\"\n",
        "        \n",
        "        # Challenge-specific analysis\n",
        "        report += \"\\n## Performance by Challenge Type\\n\"\n",
        "        \n",
        "        for challenge in successful_results['occlusion_level'].unique():\n",
        "            challenge_data = successful_results[successful_results['occlusion_level'] == challenge]\n",
        "            report += f\"\"\"\n",
        "### {challenge.title()} Scenarios\n",
        "- **Images:** {len(challenge_data)}\n",
        "- **Accuracy:** {challenge_data['exact_match'].mean():.1%}\n",
        "- **Avg Error:** {challenge_data['absolute_error'].mean():.2f}\n",
        "- **Avg Confidence:** {challenge_data['confidence'].mean():.3f}\n",
        "\"\"\"\n",
        "        \n",
        "        # Camouflage-specific insights\n",
        "        camouflage_data = successful_results[successful_results['occlusion_level'] == 'camouflage']\n",
        "        if len(camouflage_data) > 0:\n",
        "            report += f\"\"\"\n",
        "## Camouflage Analysis Deep Dive\n",
        "\n",
        "Camouflaged objects represent the ultimate challenge for VLM counting, as they test the models' ability to detect objects that are specifically evolved or designed to avoid detection.\n",
        "\n",
        "**Key Findings:**\n",
        "- **Camouflage Accuracy:** {camouflage_data['exact_match'].mean():.1%} (vs {successful_results[successful_results['occlusion_level'] != 'camouflage']['exact_match'].mean():.1%} for non-camouflage)\n",
        "- **Detection Confidence:** {camouflage_data['confidence'].mean():.3f} (vs {successful_results[successful_results['occlusion_level'] != 'camouflage']['confidence'].mean():.3f} for non-camouflage)\n",
        "- **Error Pattern:** {'Under-counting dominant' if camouflage_data['bias'].mean() < -0.5 else 'Over-counting dominant' if camouflage_data['bias'].mean() > 0.5 else 'Mixed patterns'}\n",
        "\n",
        "**Individual Cases:**\n",
        "\"\"\"\n",
        "            \n",
        "            for _, row in camouflage_data.iterrows():\n",
        "                status = \"✓ Correct\" if row['exact_match'] else \"✗ Incorrect\"\n",
        "                report += f\"- {status} | {row['model']}: {row['predicted_count']} vs {row['true_count']} | {row['description']}\\n\"\n",
        "        \n",
        "        # Error analysis\n",
        "        if 'error_type' in successful_results.columns:\n",
        "            error_dist = successful_results['error_type'].value_counts()\n",
        "            report += f\"\"\"\n",
        "## Error Pattern Analysis\n",
        "- **Exact Matches:** {error_dist.get('exact', 0)} ({error_dist.get('exact', 0)/len(successful_results):.1%})\n",
        "- **Over-counting Errors:** {error_dist.get('over_count', 0)} ({error_dist.get('over_count', 0)/len(successful_results):.1%})\n",
        "- **Under-counting Errors:** {error_dist.get('under_count', 0)} ({error_dist.get('under_count', 0)/len(successful_results):.1%})\n",
        "\"\"\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "\n",
        "## Key Insights\n",
        "\n",
        "1. **Real-world Complexity:** Natural images present significantly more challenging counting scenarios than controlled synthetic data.\n",
        "\n",
        "2. **Camouflage Challenge:** Objects that blend with their environment represent the hardest counting task, often resulting in under-counting.\n",
        "\n",
        "3. **Model Differences:** Different VLMs show varying robustness to occlusion and camouflage, with some consistently over-counting and others under-counting.\n",
        "\n",
        "4. **Confidence Calibration:** Model confidence scores may not accurately reflect counting accuracy, especially for challenging scenarios.\n",
        "\n",
        "## Research Implications\n",
        "\n",
        "- **Bias Detection:** VLMs exhibit systematic biases in counting that vary by model architecture and training.\n",
        "- **Robustness Gaps:** Current models struggle with natural occlusion and camouflage scenarios.\n",
        "- **Application Considerations:** Deployment of VLMs for counting tasks should account for these biases.\n",
        "\n",
        "## Files Generated\n",
        "- **Results CSV:** {results_filename}\n",
        "- **Analysis Visualizations:** Generated inline in notebook\n",
        "\n",
        "## Next Steps\n",
        "1. Compare with synthetic occlusion results\n",
        "2. Investigate prompt engineering improvements\n",
        "3. Test additional VLM architectures\n",
        "4. Develop bias correction techniques\n",
        "\n",
        "---\n",
        "*Generated by VLM Counting Bias Research Platform*\n",
        "\"\"\"\n",
        "    \n",
        "    # Save report\n",
        "    report_filename = f\"real_world_experiment_report_{timestamp}.md\"\n",
        "    with open(report_filename, 'w') as f:\n",
        "        f.write(report)\n",
        "    \n",
        "    print(f\"\\nExperiment report saved to: {report_filename}\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(report[:2000] + \"...\\n[Report truncated for display]\")\n",
        "    \n",
        "    # Display download links in Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"\\nDownload files:\")\n",
        "        files.download(results_filename)\n",
        "        files.download(report_filename)\n",
        "    except:\n",
        "        print(f\"\\nFiles saved locally: {results_filename}, {report_filename}\")\n",
        "\n",
        "else:\n",
        "    print(\"No results to save. Please run the experiment first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated systematic evaluation of VLM counting capabilities on real-world images, including challenging camouflage scenarios.\n",
        "\n",
        "### Key Findings:\n",
        "1. **Real-world Complexity**: Natural images with occlusion and camouflage present significantly greater challenges than synthetic scenarios\n",
        "2. **Camouflage Impact**: Objects that blend with backgrounds cause substantial under-counting in most models\n",
        "3. **Model Variability**: Different VLMs show distinct bias patterns and robustness levels\n",
        "4. **Confidence Issues**: Model confidence scores often don't correlate with actual accuracy\n",
        "\n",
        "### Research Contributions:\n",
        "- Systematic evaluation framework for VLM counting biases\n",
        "- Quantification of camouflage impact on object detection\n",
        "- Cross-model comparison of counting robustness\n",
        "- Real-world benchmark for VLM counting capabilities\n",
        "\n",
        "### Future Work:\n",
        "- Expand dataset with more diverse camouflage scenarios\n",
        "- Investigate prompt engineering techniques for improved accuracy\n",
        "- Develop bias correction methods\n",
        "- Compare with specialized object detection models\n",
        "\n",
        "This research provides valuable insights into the limitations of current VLMs and informs the development of more robust vision-language systems."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
