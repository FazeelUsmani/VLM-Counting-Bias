Vision-Language Model Counting Bias under Occlusion and Camouflage

Large vision-language models (VLMs) can describe images impressively, but they often struggle with tasks like counting objects, especially when some objects are partially hidden (occluded) or camouflaged in the background. Recent evaluations have revealed that even advanced models (e.g. GPT-4 with vision) have weaknesses in numerical reasoning tasks such as counting ￼. In particular, when objects are occluded (partially or fully hidden behind other objects), VLM performance drops significantly – for example, models perform much worse counting objects in occluded scenes, with even GPT-4V failing on such tasks ￼. This project aims to systematically evaluate and visualize these biases by testing multiple VLMs on images where objects are partially occluded or camouflaged. We will implement the experiment in Python Jupyter Notebooks (for easy step-by-step execution) and ensure everything runs both locally and on Google Colab. Below we outline the complete GitHub repository structure and components for this experiment, along with recommendations on datasets, model usage, and sample outputs.

Folder Structure

Organize the repository with a clear, logical structure so that users can easily navigate the project. A proposed top-level directory layout is:
	•	notebooks/ – Jupyter notebooks for the experiments and analysis. This will include at least two notebooks (described below) demonstrating the evaluation on synthetic and real images under occlusion/camouflage.
	•	data/ – Dataset files or data preparation scripts. This may contain small sample datasets or scripts to download larger datasets (e.g. from MS COCO or a synthetic data generator). Raw images might be too large for GitHub, so we provide download links or generation code here.
	•	results/ – Outputs from experiments, such as evaluation reports (CSV/JSON files of model counts vs ground truth) and generated figures. For example, results/count_predictions.json could store model predictions, and results/plots/ could hold visualization images.
	•	models/ – (Optional) Any custom model utilities or configuration. Since we use pre-trained models via APIs or HuggingFace, we might not store large model files here, but we can include small helper scripts or prompt templates if needed.
	•	README.md – The main documentation (project overview, setup, usage instructions, and a placeholder link for the demo video).
	•	requirements.txt – Pip requirements for running the notebooks (e.g. transformers, torch, opencv-python for image handling, etc.). This ensures reproducibility on local machines and specifies any version constraints for Colab.
	•	(Additional files like .gitignore, license, etc., can be included as needed. The structure above focuses on the key experimental components.)

This structure follows data science best practices: separating notebooks, data, and results for clarity ￼ ￼. For example, having distinct folders for notebooks and results helps keep the analysis organized and reproducible. Users (including PhD-level researchers) should be able to quickly understand where to find the data, code, and outcomes of the experiments.

Datasets and Data Preparation

We include both synthetic and real-world datasets to test object counting under difficult conditions:
	•	Synthetic Data (Occlusion Patterns or Digits): We will generate images with a known number of objects and controlled occlusion. For example, a simple synthetic dataset could consist of colored shapes or MNIST digits randomly placed on a canvas, with some shapes partially covering others. This allows us to vary the occlusion level (e.g. covering 0%, 25%, 50% of each object) and test how counting accuracy degrades as occlusion increases. The repository can include a script (or notebook section) to generate these images (e.g. overlaying black boxes or camouflage patterns to hide parts of objects). This is similar in spirit to the CAPTURe-synthetic dataset, which generated patterned images with an occluding object covering part of the pattern ￼. By having ground-truth counts for these synthetic images, we can precisely evaluate model errors at different occlusion levels.
	•	Real Images (MS COCO or Custom Selection): Real-world scenes provide complexity like varied object types, natural occlusions, and background clutter. We recommend using a subset of MS COCO (a large dataset of everyday images) focusing on images with multiple instances of the same object (for straightforward counting). For example, COCO images of people in crowds, animals in groups, or parked cars often have individuals partially occluded by others. We can curate a small set of such images and manually record their true counts for evaluation. Additionally, the project can leverage the FSC-147 dataset (a few-shot counting dataset with 6,000+ images of 147 object categories) to find images with known counts – in fact, the CAPTURe benchmark built an occluded real-image set by filtering images from FSC-147 ￼. Using real images ensures our experiment covers practical scenarios of occlusion.
	•	Camouflaged Objects: To examine the camouflage scenario (objects blending into the background), we will include a few examples where the objects are hard to distinguish from their surroundings. For instance, images of wildlife with natural camouflage (like a green snake in grass or a speckled moth on a tree bark) can test whether models undercount objects that “hide in plain sight.” There are specialized datasets for Camouflaged Object Detection (e.g. COD10K, with thousands of images of camouflaged animals) ￼, which we can tap into for sample images. In our project, we might not need a full dataset but can include a handful of curated examples (with known number of camouflaged objects) to see how the models perform. The data folder or notebook will note the source of these images and any preprocessing (like resizing or format conversion).

Data handling: Rather than storing large image files in the repo, the notebooks will provide convenient links or scripts to obtain data. For example, we may include a small download script for COCO images by ID, or use the Hugging Face datasets API to fetch the CAPTURe or COD10K samples if available. We will also provide code to generate synthetic data on the fly (ensuring the process is reproducible). All data preparation steps are documented so that a user can easily acquire or create the needed images.

Model Selection and Usage

We plan to evaluate multiple VLMs to compare their counting performance. The focus is on off-the-shelf vision-language models that can take an image and produce a text response. We will use at least the following models, which span both closed-source and open-source VLMs:
	•	GPT-4V (GPT-4 Vision): This is the vision-enabled version of GPT-4, known for strong image understanding capabilities. Since GPT-4V is accessed via OpenAI’s API (or Azure OpenAI service), we will integrate it through API calls. The notebook will accept an OpenAI API key (and endpoint if using Azure) and send the image with a prompt like “How many [objects] are in this image?” to GPT-4. Note that GPT-4’s image counting ability has limitations ￼, so it’s a crucial model for our bias evaluation. Implementation detail: using GPT-4V may require setting up a special environment or Azure credentials ￼. We will include instructions in the README for how to enable this (e.g. configuring an API key environment variable). If GPT-4V is not available to some users, our notebooks will still demonstrate the intended usage (perhaps with pseudo-calls or an example response).
	•	BLIP-2 (Bootstrapping Language-Image Pre-training 2): BLIP-2 is a state-of-the-art open-source VLM known for image-to-text tasks like captioning and VQA. We will use the Hugging Face Transformers implementation of BLIP-2 ￼, which makes it easy to get model predictions in a notebook. For example, we can load a BLIP-2 checkpoint (such as Salesforce/blip2-flan-t5-xl or similar) and use its generate() method to answer our counting questions about an image. BLIP-2 is zero-shot, so we can directly prompt it with something like “Question: How many objects are in the image? Answer:” along with the image input. Integrating BLIP-2 via HuggingFace ensures the experiment is accessible (no API key needed) and reproducible. We expect BLIP-2 to give us a baseline of how an open VLM handles counting.
	•	LLaVA (Large Language and Vision Assistant): LLaVA is an open-source multimodal model that fine-tunes LLaMA (a large language model) with vision inputs, positioned as an open alternative to GPT-4V ￼. We will include LLaVA to see how such fine-tuned models perform on counting. Using LLaVA may involve loading a model checkpoint from Hugging Face (for instance, llava-hf/llava-1.5-7b-hf which is a 7B parameter version ￼ ￼). Hugging Face provides a pipeline interface (pipeline("image-text-to-text", model="llava-hf/llava-1.5-7b-hf")) that we can use to feed in an image and a prompt, obtaining the model’s text output ￼. We will document any model-specific prompt format required (LLaVA uses a chat-style prompt with an <image> token placeholder ￼, but the HF pipeline can abstract some of this). If running LLaVA locally, a machine with a GPU and sufficient memory is needed; on Colab we might use a smaller variant or instruct users to enable a GPU runtime.

These three cover a range of model types (proprietary vs. open, different training strategies). The code is written to be modular: e.g., a function get_count(model_name, image) can handle the inference for each model (switching between API calls or local inference as needed). All models will be used in zero-shot mode (no fine-tuning), to reflect their pre-trained biases.

In summary, users will see how to use an OpenAI API for GPT-4V and Hugging Face for BLIP-2/LLaVA within the notebooks. The README will also remind to install necessary packages (like openai for API calls, and transformers>=4.35 for LLaVA support ￼).

Experiment Notebooks

We provide interactive Jupyter notebooks to run the bias experiment step by step. Each notebook is crafted to be runnable on a local machine or Colab with minimal setup. They include explanatory text for beginners and rigorous evaluations for advanced users. We propose at least two main notebooks:
	1.	notebooks/01_counting_occlusion_synthetic.ipynb: This notebook focuses on the synthetic occlusion experiment. It will:
	•	Generate or Load Synthetic Images: If using a built-in generator, the notebook creates a series of images with a configurable number of objects (e.g. colored circles or digit sprites) and varying occlusion (like adding black boxes covering parts of the image). Alternatively, if a prepared synthetic set is provided (as in CAPTURe-synthetic), it can load those images and their metadata from data/synthetic/. The ground-truth count for each image is known by construction.
	•	Run VLMs on each image: The notebook will iterate through the images and for each, query the selected models (GPT-4V, BLIP-2, LLaVA) with a prompt to count the objects. To make this efficient, we might restrict to counting one specific object type per set (so the prompt can be specific, e.g. “How many red circles are in this image?” if all objects are red circles, or just “How many objects are in the image?” if unambiguous). The inference results (model answers) are collected.
	•	Record Predictions: For each image, we store the model’s numeric answer (parsed from the text output) alongside the true count. This data is accumulated into a results table (e.g. a Pandas DataFrame). The notebook will handle parsing model outputs that might be phrased in full sentences – e.g., GPT-4V might answer “There are 5 circles.”, which we convert to integer 5.
	•	Evaluation on Synthetic Data: Using the ground truth vs predictions, we compute metrics like accuracy and error (discussed in the Evaluation section below). Because synthetic data allows controlled occlusion levels, we specifically analyze performance as a function of occlusion. For example, if we generated images with 0%, 25%, 50%, 75% occlusion coverage, the notebook will group results by these levels to see how each model’s accuracy drops as occlusion increases. This provides a quantification of the occlusion bias.
	•	Visualization (in-notebook): This notebook will also produce initial plots, such as a line chart of model accuracy vs occlusion level, and perhaps example images with the model’s count annotated. These visualizations are saved to results/ as well, and some are displayed inline for the user. (Details on visualizations in a dedicated section below.)
	2.	notebooks/02_counting_real_camouflage.ipynb: This notebook covers real-world images and includes the camouflage cases:
	•	Load Real Images: It will load a small set of real photographs (e.g. from MS COCO or FSC-147, as discussed) that feature multiple countable objects. The objects of interest and their true counts are defined beforehand (perhaps via a JSON in data/real_counts.json listing image filenames and counts). We also include one or two camouflaged-object scenes – for instance, an image where several birds are camouflaged against leaves, with the true count known.
	•	Run VLMs on Real Images: Similar to the first notebook, we loop through these real images and prompt the models to count the target objects. We might need to tailor the prompt per image or dataset. For example, if an image is of a flock of birds partially hidden behind trees, the prompt could be “How many birds can you see?”. We ensure the models know what to count either by specifying the object category in the question or using captions if available. (If using COCO, we might leverage captions or annotations to decide what to count in each image.)
	•	Collect Predictions and Evaluate: We parse the model outputs to numbers and compare with the ground truth counts for each image. The notebook computes metrics like exact match accuracy on this real image set. We pay special attention to failure cases – e.g. if a model says “I see 3 people” when there were 5, that indicates a miss (likely those partially hidden). Each such case is logged.
	•	Camouflage Analysis: For the camouflaged object examples, we discuss the model behavior: do the models miss objects that are camouflaged (likely under-counting bias), or do they hallucinate and over-count? We might find, for instance, that an image with a well-camouflaged insect yields an answer “I see 1 insect” when actually there were 2 in the image (implying the model overlooked one). These observations will be noted in the results.
	•	Visualization: This notebook will also produce visual outputs, such as marking the images with the counts. For example, we might display an image with a caption like “Model predicted: 3; Ground truth: 5” to highlight a failure. These images with annotations can be saved for the report or video. We will also aggregate results in charts if relevant (though with fewer data points than synthetic, perhaps just a summary bar chart of overall accuracy per model on this set).

Both notebooks are heavily commented and structured for beginner-friendliness – they explain what each step is doing (from loading models to parsing outputs) – but they also implement rigorous evaluation steps that a PhD-level researcher would expect (like computing error metrics, significance of differences, etc.). By running these notebooks, a user can reproduce all figures and results in the repository. The notebooks are designed to run on Colab without modification: they include commands to install any missing packages and use small enough data samples to fit Colab’s resources. For instance, we may add a snippet at top to detect Colab and optionally mount Google Drive or download the data.

Example: In the synthetic notebook, after running the models on various occlusion levels, we might print a short table of results for quick viewing:

Occlusion Level	True Count	GPT-4V Count	BLIP-2 Count	LLaVA Count
0% (no occlusion)	5	5 (✅)	5 (✅)	5 (✅)
50% occluded	5	4 (❌)	5 (✅)	3 (❌)
75% occluded	5	8 (❌)	4 (❌)	5 (✅)

This shows an example where occlusion confused GPT-4V into over-counting (maybe guessing hidden items) and LLaVA under-counted, illustrating different bias behaviors.

Evaluation Methods for Counting Accuracy

Accurately evaluating the models’ counting performance is critical. We will employ several metrics and methods to quantify performance:
	•	Exact Count Accuracy: The simplest metric – the percentage of images for which the model predicted the exact correct count. This treats counting as a classification problem (each image has a correct count, did the model get it right or not?). We will report accuracy for each model on each dataset. For instance, if out of 50 test images BLIP-2 got 30 counts exactly right, its accuracy is 60%.
	•	Mean Absolute Error (MAE): We also consider numeric error. MAE is the average of $| \text{predicted count} - \text{true count} |$ across images. This gives a sense of how far off the predictions are on average. A low MAE means the model is usually close to the correct number even if not exactly right. We might use this especially if the counts vary widely in range.
	•	Symmetric Mean Absolute Percentage Error (sMAPE): This is a more specialized metric (used in the CAPTURe paper ￼) that measures error as a percentage of the true count, treating overestimation and underestimation symmetrically. sMAPE is defined as $\frac{100%}{N}\sum_{i=1}^{N}\frac{|P_i - T_i|}{(P_i + T_i)/2}$ where $P_i$ is predicted count and $T_i$ true count. It’s useful to compare performance across different scales of counts. We will calculate sMAPE for completeness, and it helps if, say, an error of 2 when the true count was 4 (50% off) is worse (in percentage terms) than an error of 2 when the true count was 50 (only 4% off).
	•	Under-count vs Over-count Analysis: To uncover bias patterns, we will track whether each model tends to under-count (predict fewer objects than present) or over-count. For each image, we can compute error = prediction - truth. We will aggregate the distribution of these errors. For example, we might find GPT-4V often has a positive error on heavily occluded images (meaning it guesses extra objects behind the occluder – an over-count bias) ￼, whereas BLIP-2 might have negative error (missing objects it didn’t detect). We’ll quantify this by reporting the fraction of images each model under-counted, got correct, or over-counted. This is important for bias: a consistent skew in one direction indicates a systematic bias.
	•	Occlusion Level vs Accuracy: For the synthetic experiment, we explicitly evaluate performance at each occlusion level. We will produce a small table or chart of accuracy at 0% occlusion, 25%, 50%, etc., for each model. This gives a trend line of how robustness degrades as occlusion increases. Similarly, if we define “camouflage difficulty” (perhaps qualitatively or by an image’s camouflage index), we note performance on those vs normal images. A trend analysis helps demonstrate bias: e.g., “Model X’s accuracy drops from 90% with no occlusion to 40% with heavy occlusion”, whereas a human’s performance might remain high ￼.
	•	Statistical Rigor: Although this is a small-scale experiment, we ensure rigor by, for instance, averaging results over multiple images per condition and possibly computing standard deviations. If feasible, we might run each model multiple times on the same prompt (to see if there’s any randomness, though for deterministic VLMs there might not be). We also consider the practical significance: for instance, if one model has higher accuracy than another on occluded images, we’ll mention it but also note any caveats (like different model sizes or the limited sample size of our test).

All evaluation code is contained either in the notebooks or in a shared utility function. The results of these metrics are saved in machine-readable form (CSV/JSON) in the results/ folder for further analysis or plotting.

Visualizations and Analysis Outputs

Visualizing the results will make it easier to understand the models’ biases. We plan several visualization outputs to be generated by the notebooks:
	•	Count Error Heatmap: We will create a heatmap-style confusion matrix comparing predicted counts vs. actual counts. The axes would be true count and predicted count, and each cell’s color intensity shows how many images fell into that case. This is especially insightful for synthetic data where the true counts are controlled. For example, if the true count is 5 for many images, we might see a darker cell at (true=5, pred=5) for one model, but another model might show darker cells at (5,4) and (5,6) indicating frequent off-by-one errors. Such a heatmap helps quickly spot biases like always under-counting high quantities. We can plot a separate matrix for each model, or a combined one highlighting differences.
	•	Occlusion vs Performance Plot: A line chart or bar chart will illustrate the bias trend across occlusion levels. In a simple form, we plot occlusion percentage on the x-axis and the model’s accuracy (or error) on the y-axis, with one line per model. For example, below is a conceptual chart showing accuracy dropping as occlusion increases for three models:

Example chart: Counting accuracy of different VLMs vs. occlusion level. As occlusion (hidden portion of objects) increases, all models show a decline in accuracy, indicating a bias/weakness in handling occluded objects. In this illustrative example, GPT-4V performs best overall but still drops significantly at high occlusion, while the open-source LLaVA model falls off faster.

This kind of visualization will be generated from our synthetic experiment data. It clearly demonstrates the impact of occlusion on each model and will be a highlight in our analysis (e.g., we expect to confirm research findings that even the strongest VLM’s accuracy deteriorates with occlusion ￼).
	•	Failure Case Examples: We will include annotated images to showcase typical failure cases. A powerful way to present this is to take an image where a model miscounted, and overlay the image with the model’s prediction and the correct answer. For instance, consider an image of a pattern of cups partly hidden by a box. GPT-4V might answer “21 cups” when only 16 are present. We can display that image with a note of predicted vs actual counts for clarity. An example from a recent occluded counting benchmark is shown below, where GPT-4 (vision) overestimates the number of cups hidden behind a black square ￼:

Illustrative failure case: An occluded object counting scenario. The black square hides part of a regular pattern of cups. A human reasoned that there are 4 rows × 4 columns = 16 cups in total (accounting for the hidden ones). GPT-4V, however, miscounted and guessed 21 cups, demonstrating how occlusion can confuse the model’s world understanding.

Such images will be generated or taken from our experiment results and included in the analysis. We will create a small gallery of failure cases in the notebook or in an output folder (with images named like failure_gpt4v_image3.png etc.). Capturing these cases visually is important for a qualitative understanding of bias: it allows us to ask why the model failed (did it double-count something? miss something invisible? etc.). For camouflage, a failure might be an image of a camouflaged animal where the model only spotted one out of two animals – we’d highlight the missed animal in the visualization.
	•	Comparison Charts: We may also include a summary bar chart comparing models. For example, a grouped bar chart for “Accuracy on unobscured vs obscured images” for each model could be effective. Each model would have two bars: one for clear images, one for occluded images, showing the drop. Similarly, a bar chart of overall accuracy for each model on the real dataset vs synthetic dataset could be shown. These give at-a-glance comparisons that can impress experts (by summarizing performance differences) yet are simple enough for a general audience to understand.
	•	Bias Illustration: If our data shows a particular bias (like systematically under-counting when occlusion >50%), we will visualize that. For instance, a pie chart could show percentage of times each model under-counted, over-counted, or was correct. Or a histogram of the error values for each model. These visualizations back up statements like “Model A tends to under-count objects in crowded scenes, whereas Model B sometimes over-counts by hallucinating hidden objects.”

All visualizations will be saved in the results/ folder (as PNG or PDF files). In the README or notebooks, we will refer to these figures when discussing findings. The visual approach, combined with the quantitative metrics, provides a comprehensive analysis of counting bias under occlusion/camouflage – suitable for a rigorous report. We ensure each figure has clear labels and legends so that even a beginner can interpret them (and we include descriptive captions in the notebooks).

Exportable Results and Reports

Beyond interactive analysis, the project will output exportable results so that others can easily examine or reuse the data:
	•	CSV/JSON Prediction Logs: After running the experiments, we save a structured file (CSV or JSON) containing each image’s results. Each row/entry will include fields such as: image_id, dataset (synthetic or COCO, etc.), true_count, pred_gpt4v, pred_blip2, pred_llava. This allows for offline analysis or plotting. For example, a CSV line might look like:
image_0001_synthetic, true_count=8, gpt4v=10, blip2=8, llava=7
indicating GPT-4V overcounted that image. Such a file can be loaded by others (say, in Excel or Python) to verify our calculations or to apply different analysis. We will likely maintain one results file per experiment (e.g., results/synthetic_counts.json and results/real_counts.json).
	•	Aggregate Metrics Report: We can also output a summary JSON with overall metrics. For instance:

{
  "GPT-4V": {"accuracy": 0.60, "MAE": 1.5, "under_count_rate": 0.30, "over_count_rate": 0.10},
  "BLIP-2": {"accuracy": 0.55, "MAE": 1.8, ...},
  "LLaVA": {"accuracy": 0.50, ...}
}

This provides a quick view of each model’s performance. Storing it in a machine-readable form means it can be versioned or compared if we improve the models or add more data later.

	•	Visualizations and Plots: As described, plots will be saved to files (e.g., results/plots/occlusion_accuracy.png). We ensure these are of decent resolution for use in presentations or the demo video. For instance, the line chart of accuracy vs occlusion would be saved, as well as any confusion matrix heatmaps (perhaps as heatmap_blip2.png, etc.). By exporting them, we allow the README or external docs to embed these images, and viewers of the GitHub can quickly see outcomes without running the code.
	•	Reproducibility notes: The README will clarify that due to randomness (some models might have nondeterministic outputs or API may vary), exact results might differ slightly per run, but overall trends should hold. If a particular run is noteworthy, we might include a saved snapshot of results (for example, the JSON from the run we discuss in the README, to ensure consistency with reported numbers).

In summary, the repository doesn’t just show the results in notebooks; it packages the results in accessible formats (CSV/JSON and PNG charts). This makes the project’s findings easy to share and verify, aligning with reproducible research standards. A researcher could, for instance, take our CSV of predictions and run their own statistical tests on it without needing to run the models again.

README.md Overview and Instructions

The README.md at the project root will serve as a comprehensive guide for the repository. It will be written in a clear, welcoming tone, assuming the reader might be a first-time user of such models but also including technical details for the expert. Key sections of the README will include:
	•	Project Introduction: A summary of what the project is about – e.g., “This repository contains code and data for evaluating bias in object counting by Vision-Language Models under challenging conditions (occlusion and camouflage). We test models like GPT-4V, BLIP-2, and LLaVA on synthetic and real images to analyze how occlusions or background camouflage affect their counting accuracy.” This recaps the motivation (possibly mentioning that counting occluded objects is a known challenge ￼). We’ll also credit any inspirations (for example, mentioning that it builds on ideas from CAPTURe benchmark for occluded counting ￼).
	•	Folder Structure Explanation: We will list the main folders (notebooks, data, results, etc.) and briefly describe their contents, similar to how we outlined above. This helps users navigate the repo without having to open each folder. (We can largely reuse the descriptions from the “Folder Structure” section in this answer, adjusted for context.)
	•	Setup Instructions: Step-by-step instructions to get the environment ready. This includes:
	•	Installing required packages (pip install -r requirements.txt). We note any particular versions needed (especially for HuggingFace transformers or if using PyTorch).
	•	Instructions for running in Google Colab: We might provide Colab badges or links that open the notebooks directly in Colab. This way, a user can click and start running without worrying about local setup. We’ll ensure these links are included for each main notebook.
	•	Any needed API keys: We’ll mention that to use GPT-4V, one must have an OpenAI API key or Azure key. The README will describe how to supply it (for example, by editing a cell in the notebook or setting it as an environment variable). We will caution that without this key, the GPT-4V parts will be skipped or dummy (so the user isn’t stuck).
	•	Data downloading: If the notebooks require the user to download data (e.g., COCO images), the README will explain how. Possibly we’ll include a script or an automated download in the notebook, but we’ll double-check the steps here. For example: “The first time you run the real images notebook, run the Download COCO samples cell which uses the COCO API to fetch images, or manually place images in data/real/ as per the provided list.”
	•	Running the Experiments: We will explain how to run the notebooks in order:
	1.	Run the synthetic occlusion notebook to reproduce Figures X and Y.
	2.	Run the real images notebook for Figures Z, etc.
It’s important to note if running one depends on outputs of another (in our case they are independent, so just order is logical but not mandatory). We will also mention approximate runtime and resource needs: e.g., “Running all models on 50 images might take around 10 minutes on a GPU runtime. Without a GPU, BLIP-2 and LLaVA will be slower.” If any notebook is particularly heavy, we advise enabling Colab Pro or reducing the dataset size as needed.
	•	Results and How to Interpret: After instructions, the README will highlight what the user should expect. For instance, “After running, you will find output files in results/ and sample plots in results/plots/. Key findings include the accuracy drop with occlusion (see occlusion_accuracy.png) and examples of model mistakes (see the images in results/failures/).” We might even embed a couple of the saved images in the README to provide a quick visual summary. According to good README practices, we’ll ensure it answers the What, Why, and How of the project clearly ￼.
	•	Demo Video Link: We will add a section, “Demo Video”, in the README. Since the user requested Loom or Zoom demo placeholders, we can include something like: “Demo Video – A 5-minute walkthrough of the project (to be added).” If the video is ready, this would be a hyperlink (e.g., to a Loom recording). For now, we’ll label it as a placeholder. The idea is that anyone visiting the repo can easily find the video overview. This is especially helpful for visually walking through results, which complements the written documentation.
	•	References and Credits: To maintain academic rigor, we will cite relevant work (for example, we may reference the CAPTURe paper for occluded counting ￼ and any sources of datasets or models). Also, we’ll thank any open-source projects or libraries used. If the user is from MIT/Purdue, they might appreciate the acknowledgment of sources.

Overall, the README will be beginner-friendly (explaining acronyms like VLM, clarifying occlusion/camouflage meaning briefly, giving clear steps) but also convey the depth of the experiment so that an expert recognizes the thoroughness. It acts as the primary guide to use and understand the repository.

Demo Video Script Outline (5–10 min)

Finally, we prepare an outline for a short demo video (around 5–10 minutes) that walks through the project’s notebook and findings. The video is meant to be accessible but also showcase the technical rigor. Below is a structured script/outline for the video:
	1.	Introduction (0:00–1:00): Start by introducing the topic and motivation. “Hello, and welcome! In this demo, we will explore how vision-language models count objects in images, especially when some objects are hidden or camouflaged. Counting might seem easy for us, but as research shows, even advanced AI models struggle with occluded objects ￼. This project investigates that issue.” Mention the models (GPT-4V, BLIP-2, LLaVA) and the goal of the experiment. Keep it brief and engaging for a general audience, but mention that we have a rigorous setup behind the scenes (to hook the experts).
	2.	Project Overview and Setup (1:00–2:00): Show the GitHub repository structure on screen. Highlight the notebooks/ folder and open the first notebook. “The repository is organized for clarity – we have Jupyter notebooks for the experiments, a data folder for images, and a results folder for outputs. This makes it easy to follow along and reproduce the results.” If using Loom/Zoom screen share, navigate through README for a few seconds, pointing out the instructions and the fact that one can run this on Colab or locally.
	3.	Running the Synthetic Occlusion Notebook (2:00–5:00): Dive into 01_counting_occlusion_synthetic.ipynb. Walk through the notebook steps:
	•	Show how we generate a sample synthetic image (perhaps even display one in the video) and explain the scenario: “Here we have a grid of objects; some are hidden behind this black box. The true total is, say, 16. Let’s see what each model predicts.” Run the cell that calls the models. As it runs (we might pre-record this to skip wait times), describe what’s happening (calling APIs or model inference).
	•	Once the results are in, show a snippet of the output. For example, highlight that GPT-4V said “21” here, overshooting the count, while BLIP-2 got it right at 16. You can overlay or zoom into the notebook output for clarity.
	•	Show a visualization from this notebook: e.g., present the line chart of accuracy vs occlusion that was generated. “Look at this plot: on the x-axis we increased occlusion, and on the y-axis is accuracy. All models drop in accuracy as more of the objects are hidden. GPT-4V (blue line) starts high but falls off, and by 80% occlusion it’s below 50% accuracy. The others degrade even sooner. This quantifies the bias we suspected – occlusion really trips up these models.” Such commentary demonstrates a rigorous insight in a simple visual way.
	•	If time permits, mention the confusion matrix heatmap or an error analysis (maybe just verbally: “We also analyzed whether models tend to under-count or over-count. It turns out GPT-4V often overestimates hidden objects – as if it’s hallucinating extras – whereas BLIP-2 usually underestimates when it can’t see everything.”).
	4.	Real Images and Camouflage Notebook (5:00–7:00): Switch to 02_counting_real_camouflage.ipynb. “Now let’s test on some real images.” Show an example image from the dataset (maybe briefly display it). For instance, “Here’s a photo from COCO: a group of ducks by a pond, with some partially behind others. We ask the models to count the ducks.” Run the models and reveal their answers: perhaps GPT-4V says 5 while the actual is 6, BLIP-2 says 4, etc. Explain the results: “Both models missed at least one duck hidden in the back. As expected, occlusion in real scenes is also challenging.”
	•	Show a camouflage example image if available: e.g., an insect on leaves. “This image has two leaf insects camouflaged. A person might spot both after careful look. The models? GPT-4V only found one, and BLIP-2 also said one. They didn’t notice the second insect at all.” This emphasizes the camouflage point.
	•	Present any chart or summary from this notebook, like a bar chart of overall accuracy on the real set. “Overall, GPT-4V was correct on 70% of the clear images but only 40% of the occluded/camouflaged ones – a big drop. The open models were even less accurate. This aligns with our synthetic test findings and confirms the bias on real data.” If we have an example failure image annotated (from the failure cases), flash it on screen and narrate what went wrong, to leave a strong visual impression of model limitations.
	5.	Key Findings and Takeaways (7:00–8:30): Summarize what we learned. “In summary, we found that occlusion and camouflage significantly reduce counting accuracy for current VLMs. Even a powerful model like GPT-4V, which can describe images in detail, failed to correctly count objects when some were hidden – in one case guessing 21 cups instead of 16 ￼. The open-source models struggled even more, often missing hidden items. This suggests that these models lack a robust spatial reasoning or world-model to infer hidden content.” Also mention any positive note: “On simple images with no occlusion, the models did quite well, so it’s specifically the occlusion/camouflage that causes the drop – a clear bias in their performance.” This conclusion is both understandable to a layperson (models fail when things are hidden) and interesting to an expert (it underscores a known gap in VLM capabilities).
	6.	Future Work or Closing (8:30–9:30): Conclude the video by hinting at what one could do next or why this matters. “This experiment is a starting point – improving counting under occlusion is an active research area ￼ ￼. Future work could involve fine-tuning models with more spatial reasoning or using auxiliary tools (like depth predictors or segmentation) to help the models. For users, our repository provides a reproducible setup – you can try more images, or plug in new models as they come out.” Thank the viewer. “Thank you for watching this demo. Check out the GitHub repo (link below) to explore the notebooks yourself!”

Throughout the video, the tone remains educational and engaging. We avoid heavy jargon unless we’ve explained it (e.g., we’d say “occlusion – meaning some objects are hidden” the first time). We balance this with technical accuracy – for instance, mentioning that we computed error rates or that these models were used zero-shot. The screen recording focuses on showing the notebook outputs and key plots, rather than code walls (except when briefly showing how easy it is to call the models). By the end, a viewer from MIT or Purdue should be nodding at the thoroughness (we covered data, methodology, metrics, references to literature) while a beginner should still grasp the main points (pictures with hidden objects confuse the AI, here’s evidence).

Finally, the README’s demo video section will link to this recording so that others can easily find it. The combination of the well-documented repo and the walkthrough video makes the project accessible and impressive to a wide range of audiences.

Sources: The design and findings are informed by recent research on VLM counting and occlusion ￼ ￼, and we utilize open-source models and datasets (Hugging Face, MS COCO, etc.) in line with their recommended usage ￼ ￼. This ensures that our experiment is grounded in current state-of-the-art practices and addresses a known challenge in computer vision and AI reasoning.